{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom hashlib import sha256\nsns.set()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5402661119519f4267bfbd3d53713ed2c1aa73bb"
      },
      "cell_type": "markdown",
      "source": "## Data Loading and Cleaning"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dbafb8dcf1af4e7d9c3b4bf16161327a6ae695dd"
      },
      "cell_type": "code",
      "source": "# Load applications data\ntrain = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "569b63d0d9d83e59de73fb8dbcbf98c70a51ad7f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "c83bc5b0363e361864d48367e1b416fa37254300",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Print info about each column in the train dataset\nfor col in train:\n    print(col)\n    Nnan = train[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/train.shape[0])\n    print(train[col].describe())\n    if train[col].dtype==object:\n        print('Categories and Count:')\n        print(train[col].value_counts().to_string(header=None))\n    print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da59786e9376f69b3b179ba596428f12818a5ad4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Print info about each column in the test dataset\nfor col in test:\n    print(col)\n    Nnan = test[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/test.shape[0])\n    print(test[col].describe())\n    if test[col].dtype==object:\n        print('Categories and Count:')\n        print(test[col].value_counts().to_string(header=None))\n    print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "33f2e6988f3fb8f05023b55a6a9d74e47bc67958"
      },
      "cell_type": "markdown",
      "source": "The column containing the values we are trying to predict, `TARGET`, doesn't contain any missing values.  The value of `TARGET` is $0$ when the loan was repayed sucessfully, and $1$ when there were problems repaying the loan.  Many more loans were succesfully repayed than not, which means that the dataset is imbalanced in terms of our dependent variable, which is something we'll have to watch out for when we build a predictive model later:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d73aae2ef303cdce678f87bc49c164073f26b081",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Show target distribution\ntrain['TARGET'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae4313940302e607227540defc3302c6a32d856c"
      },
      "cell_type": "markdown",
      "source": "There's a lot of categorical columns - let's check that, for each column, all the categories we see in the training set we also see in the test set, and vice-versa."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc7177f28a14c85162c608d9e85bee10ffe432a4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for col in test:\n    if test[col].dtype==object:\n        print(col)\n        print('Num Unique in Train:', train[col].nunique())\n        print('Num Unique in Test: ', test[col].nunique())\n        print('Unique in Train:', sorted([str(e) for e in train[col].unique().tolist()]))\n        print('Unique in Test: ', sorted([str(e) for e in test[col].unique().tolist()]))\n        print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "17020939b123b2d49ac558c0ef14e6ef6da4bfb9"
      },
      "cell_type": "markdown",
      "source": "The gender column contains whether the loan applicant was male or female.  The training datset contains 4 values which weren't empty but were labelled `XNA`.  Normally we would want to create a new column to represent when the gender value is null.  However,  since the test dataset has only `M` and `F` entries, and because there are only 4 entries with a gender of `XNA` in the training set, we'll remove those entries from the training set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "734bd2ec150be7b5a1764f1bda3e0ced9dcd7e96",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Remove entries with gender = XNA\ntrain = train[train['CODE_GENDER'] != 'XNA']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88a9836bde9985a765da6194384abc5b76273371"
      },
      "cell_type": "markdown",
      "source": "The `NAME_INCOME_TYPE` column also contained entries for applicants who were on Maternity leave, but no such applicants were in the test set.  There were only 5 such applicants in the training set, so we'll remove these from the training set."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "aa8f509cda7c020400a0c5718d6afe30bc17dd2c"
      },
      "cell_type": "code",
      "source": "# Remove entries with income type = maternity leave\ntrain = train[train['NAME_INCOME_TYPE'] != 'Maternity leave']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c249acebaa36a060854c7d624ef765342fc41366"
      },
      "cell_type": "markdown",
      "source": "Similarly, in the `NAME_FAMILY_STATUS` column, there were 2 entries in the training set with values of `Unknown`, and no entries with that value in the test set.  So, we'll remove those too."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "340b9ffbae4a9fe9762c12944f24e5bef62cf478"
      },
      "cell_type": "code",
      "source": "# Remove entries with unknown family status\ntrain = train[train['NAME_FAMILY_STATUS'] != 'Unknown']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1abcf81abdd12bac518b9ba6dbfae5549920d9b5"
      },
      "cell_type": "markdown",
      "source": "There were some funky values in the `DAYS_EMPLOYED` column:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c77e621099e7159f337fe6c498156658b3ab1ef3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train['DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED')\nplt.ylabel('Count')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "18f7109b6142ea10aba85c8b66f70b8c87ada27d"
      },
      "cell_type": "markdown",
      "source": "$350,000$ days?  That's like $1,000$ years!  Looks like all the reasonable values represent the number of days between when the applicant was employed and the date of the loan application.  The unreasonable values are all exactly $365,243$, so we'll set those to `NaN`."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88c253d19d78167320e5c5f6d802c73e8d88e74b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Show distribution of reasonable values\ntrain.loc[train['DAYS_EMPLOYED']<200000, 'DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED (which are less than 200,000)')\nplt.ylabel('Count')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbc994f55797034195413a8220f94a6316e34839",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Show all unique outlier values\ntrain.loc[train['DAYS_EMPLOYED']>200000, 'DAYS_EMPLOYED'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b3b65921e206aba230ddb2c63c6e1eeb52352bfd"
      },
      "cell_type": "code",
      "source": "# Set unreasonable values to nan\ntrain.loc[train['DAYS_EMPLOYED']==365243, 'DAYS_EMPLOYED'] = np.nan\ntest.loc[test['DAYS_EMPLOYED']==365243, 'DAYS_EMPLOYED'] = np.nan",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1494ed2198f37d78528546fb590bbba561fc8df2"
      },
      "cell_type": "markdown",
      "source": "The column `WEEKDAY_APPR_PROCESS_START` contains categorical information corresponding to the day of the week.  We could encode these categories as the values 1-7, but this would imply that Sunday and Monday are more similar than, say Tuesday and Sunday.  We could also one-hot encode the column into 7 new columns, but that would create 7 additional dimensions.  Seeing as the week is cyclical, we'll encode this information into two dimensions by encoding them using polar coordinates.  That is, we'll represent the days of the week as a circle.  That way, we can encode the days of the week independently, but only add two dimensions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d86fc449a9ae5faa208bd35dc91cd509c5334a0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Create map from categories to polar projection\nDOW_map = {\n    'MONDAY':    0,\n    'TUESDAY':   1,\n    'WEDNESDAY': 2,\n    'THURSDAY':  3,\n    'FRIDAY':    4,\n    'SATURDAY':  5,\n    'SUNDAY':    6,\n}\nDOW_map1 = {k: np.cos(2*np.pi*v/7.0) for k, v in DOW_map.items()}\nDOW_map2 = {k: np.sin(2*np.pi*v/7.0) for k, v in DOW_map.items()}\n\n# Show encoding of days of week -> circle\ndays = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']\ntt = np.linspace(0, 2*np.pi, 200)\nxx = np.cos(tt)\nyy = np.sin(tt)\nplt.plot(xx,yy)\nplt.gca().axis('equal')\nplt.xlabel('Encoded Dimension 1')\nplt.ylabel('Encoded Dimension 2')\nplt.title('2D Projection of days of the week')\nfor day in days:\n    plt.text(DOW_map1[day], DOW_map2[day], day, ha='center')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ac9340cde811b82e637d51d57b3f2c3e6c07655",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# WEEKDAY_APPR_PROCESS_START to polar coords\ncol = 'WEEKDAY_APPR_PROCESS_START'\ntrain[col+'_1'] = train[col].map(DOW_map1)\ntrain[col+'_2'] = train[col].map(DOW_map2)\ntrain.drop(columns=col, inplace=True)\ntest[col+'_1'] = test[col].map(DOW_map1)\ntest[col+'_2'] = test[col].map(DOW_map2)\ntest.drop(columns=col, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08fa08c097ace5bda19092d7250fa445b89217fb"
      },
      "cell_type": "markdown",
      "source": "For the housing-related features (e.g. `LIVINGAPARTMENTS_MODE`, `BASEMENTAREA_AVG`, etc) there are combinations of some PREFIX (e.g. `LIVINGAPARTMENTS`,  `BASEMENTAREA`, etc) and some POSTFIX (e.g. `MODE`, `MEDI`, `AVG`, etc) into a variable `PREFIX_POSTFIX`.  However, if one value for a given PREFIX is empty, the other values for that PREFIX will also be empty.  \n\nFor each column which has some empty values, we want to add an indicator column which is 1 if the value in the corresponding column is empty, and 0 otherwise.  However, if we do this with the housing-related features, we'll end up with a bunch of duplicate columns!  This is because the same samples have null values across all the POSTFIX columns for a given PREFIX.   The same problem crops up with the CREDIT_BUREAU-related features. To handle this problem, after creating the null indicator columns, we'll check for duplicate columns and merge them.\n\nSo, first we'll add columns to indicate where there are empty values in each other column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02acbadff9f44a022fc008c8aeba9ba72b8bd553",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Add indicator columns for empty values\nfor col in test:\n    train_null = train[col].isnull()\n    test_null = test[col].isnull()\n    if (train_null | test_null).sum()>0:\n        train[col+'_ISNULL'] = train_null\n        test[col+'_ISNULL'] = test_null",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d64ad0c6824827fae7a5a04fce45d7768360d71"
      },
      "cell_type": "markdown",
      "source": "Then we can label encode categorical features with only 2 possible values (that is, turn the labels into either 0 or 1)."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0167a10c72607282e6c6722d829be3beb146cc06"
      },
      "cell_type": "code",
      "source": "# Label encoder\nle = LabelEncoder()\n\n# Label encode binary fearures in training set\nfor col in train: \n    if train[col].dtype==object and train[col].nunique()==2:\n        if col+'_ISNULL' in train.columns: #missing values here?\n            train.loc[train[col+'_ISNULL'], col] = 'NaN'\n        train[col] = le.fit_transform(train[col])\n        if col+'_ISNULL' in train.columns: #re-remove missing vals\n            train.loc[train[col+'_ISNULL'], col] = np.nan\n            \n# Label encode binary fearures in test set\nfor col in test: \n    if test[col].dtype==object and test[col].nunique()==2:\n        if col+'_ISNULL' in test.columns: #missing values here?\n            test.loc[test[col+'_ISNULL'], col] = 'NaN'\n        test[col] = le.fit_transform(test[col])\n        if col+'_ISNULL' in test.columns: #re-remove missing vals\n            test.loc[test[col+'_ISNULL'], col] = np.nan",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e0627f1939645184f0abc88b39f13367d92264a5"
      },
      "cell_type": "markdown",
      "source": "Then we'll one-hot encode the categorical features which have more than 2 possible values."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "bff906ff8e92bbf846d43e59fcb0b01150c1545f"
      },
      "cell_type": "code",
      "source": "# Get categorical features to encode\ncat_features = []\nfor col in train: \n    if train[col].dtype==object and train[col].nunique()>2:\n        cat_features.append(col)\n\n# One-hot encode categorical features in train set\ntrain = pd.get_dummies(train, columns=cat_features)\n\n# One-hot encode categorical features in test set\ntest = pd.get_dummies(test, columns=cat_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3d531e3fb93e4eddc167aba65efad839f68a6774"
      },
      "cell_type": "markdown",
      "source": "And finally we'll remove duplicate columns.  We'll hash the columns and check if the hashes match before checking if all the values actually match, because it's a lot faster than comparing $O(N^2)$ columns elementwise."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "9f97dbdb0e4ec6885e533d34891573d57b2b3a73",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Hash columns\nhashes = dict()\nfor col in train:\n    hashes[col] = sha256(train[col].values).hexdigest()\n    \n# Get list of duplicate column lists\nNcol = train.shape[1] #number of columns\ndup_list = []\ndup_labels = -np.ones(Ncol)\nfor i1 in range(Ncol):\n    if dup_labels[i1]<0: #if not already merged,\n        col1 = train.columns[i1]\n        t_dup = [] #list of duplicates matching col1\n        for i2 in range(i1+1, Ncol):\n            col2 = train.columns[i2]\n            if ( dup_labels[i2]<0 #not already merged\n                 and hashes[col1]==hashes[col2] #hashes match\n                 and train[col1].equals(train[col2])): #cols are equal\n                #then this is actually a duplicate\n                t_dup.append(col2)\n                dup_labels[i2] = i1\n        if len(t_dup)>0: #duplicates of col1 were found!\n            t_dup.append(col1)\n            dup_list.append(t_dup)\n        \n# Merge duplicate columns\nfor iM in range(len(dup_list)):\n    new_name = 'Merged'+str(iM)\n    train[new_name] = train[dup_list[iM][0]].copy()\n    test[new_name] = test[dup_list[iM][0]].copy()\n    train.drop(columns=dup_list[iM], inplace=True)\n    test.drop(columns=dup_list[iM], inplace=True)\n    print('Merged', dup_list[iM], 'into', new_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d06d46ac51599ae674c72da0484bbd29a46e2d96"
      },
      "cell_type": "code",
      "source": "# Ensure all data is stored as floats\ntrain = train.astype(np.float32)\ntest = test.astype(np.float32)\n\n# Target labels\ntrain_y = train['TARGET']\n\n# Align test and training data\ntrain, test = train.align(test, join='inner', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee31921e6399f54a87822d71a3c3875ea0f9f862",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(train.shape)\nprint(test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce24926c749f1eff08f211b8d585a3b6bcfdd89c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Classification pipeline\nxgb_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', XGBClassifier())\n])\n\n# Cross-validated AUROC?\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\nscores = cross_val_score(xgb_pipeline, train, train_y, \n                         cv=3, scoring=auroc_scorer)\nprint(scores)\nprint('Mean AUROC:', scores.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "7717ceceb1b06c890f45c53719bc5f8904d88f87"
      },
      "cell_type": "code",
      "source": "# Fit to training data\nxgb_fit = xgb_pipeline.fit(train, train_y)\n\n# Predict default probabilities of test data\ntest_pred = xgb_fit.predict_proba(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d698a7e393bfaa81067072b47344cf31e477e828"
      },
      "cell_type": "code",
      "source": "# Save predictions to file\noutput = test[['SK_ID_CURR']]\noutput['TARGET'] = test_pred\noutput.to_csv('xgboost_baseline.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "Notes:\n\nFeature selection with Mutual information?  Featuretools?\n\ninteraction terms and keep ones w/ high MI?  Poly features shouldn't be needed for tree method... but would want em if doing logistic reg.\n\nGonna have to do oversampling/undersampling to account for that class imbalance\n\nXGboost\n\nCALIBRATION!\n\nBayesian paramater optimization?"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}