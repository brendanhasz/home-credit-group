{
  "cells": [
    {
      "metadata": {
        "_uuid": "be4820cca6d91fa5b86e64729a28be8529eb4b9d"
      },
      "cell_type": "markdown",
      "source": "# Home Credit Group Loan Risk Prediction\n\nTODO: intro\n\n"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import auc, roc_curve, roc_auc_score, make_scorer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom xgboost import XGBClassifier\nfrom hashlib import sha256\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import make_pipeline\nimport time\nimport featuretools as ft\nfrom featuretools import selection \nimport gc\nsns.set()",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5402661119519f4267bfbd3d53713ed2c1aa73bb"
      },
      "cell_type": "markdown",
      "source": "## Data Loading and Cleaning\n\nFirst, let's load all of the data files:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbafb8dcf1af4e7d9c3b4bf16161327a6ae695dd"
      },
      "cell_type": "code",
      "source": "# Load applications data\ntrain = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\ncash_balance = pd.read_csv('../input/POS_CASH_balance.csv')\ncard_balance = pd.read_csv('../input/credit_card_balance.csv')\nprev_app = pd.read_csv('../input/previous_application.csv')\npayments = pd.read_csv('../input/installments_payments.csv')",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "779c7dcafde8ba753a608e8d3530aff387e7eb8d"
      },
      "cell_type": "code",
      "source": "#JUST FOR TESTING\n\ntrain = train.loc[:10000,:]\ntest = test.loc[:10000,:]\nbureau = bureau.loc[:10000,:]\nbureau_balance = bureau_balance.loc[:10000,:]\ncash_balance = cash_balance.loc[:10000,:]\ncard_balance = card_balance.loc[:10000,:]\nprev_app = prev_app.loc[:10000,:]\npayments = payments.loc[:10000,:]\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "64c8ad85ea70fbcbc4474b0000ae9a6c39645167"
      },
      "cell_type": "markdown",
      "source": "And now we can take a look at the data we're working with.  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "569b63d0d9d83e59de73fb8dbcbf98c70a51ad7f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "c83bc5b0363e361864d48367e1b416fa37254300"
      },
      "cell_type": "code",
      "source": "# Print info about each column in the train dataset\n\"\"\"\nfor col in train:\n    print(col)\n    Nnan = train[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/train.shape[0])\n    print(train[col].describe())\n    if train[col].dtype==object:\n        print('Categories and Count:')\n        print(train[col].value_counts().to_string(header=None))\n    print()\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da59786e9376f69b3b179ba596428f12818a5ad4"
      },
      "cell_type": "code",
      "source": "# Print info about each column in the test dataset\n\"\"\"\nfor col in test:\n    print(col)\n    Nnan = test[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/test.shape[0])\n    print(test[col].describe())\n    if test[col].dtype==object:\n        print('Categories and Count:')\n        print(test[col].value_counts().to_string(header=None))\n    print()\n\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "33f2e6988f3fb8f05023b55a6a9d74e47bc67958"
      },
      "cell_type": "markdown",
      "source": "The column containing the values we are trying to predict, `TARGET`, doesn't contain any missing values.  The value of `TARGET` is $0$ when the loan was repayed sucessfully, and $1$ when there were problems repaying the loan.  Many more loans were succesfully repayed than not, which means that the dataset is imbalanced in terms of our dependent variable, which is something we'll have to watch out for when we build a predictive model later:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d73aae2ef303cdce678f87bc49c164073f26b081"
      },
      "cell_type": "code",
      "source": "# Show target distribution\ntrain['TARGET'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae4313940302e607227540defc3302c6a32d856c"
      },
      "cell_type": "markdown",
      "source": "There's a lot of categorical columns - let's check that, for each column, all the categories we see in the training set we also see in the test set, and vice-versa."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc7177f28a14c85162c608d9e85bee10ffe432a4"
      },
      "cell_type": "code",
      "source": "for col in test:\n    if test[col].dtype==object:\n        print(col)\n        print('Num Unique in Train:', train[col].nunique())\n        print('Num Unique in Test: ', test[col].nunique())\n        print('Unique in Train:', sorted([str(e) for e in train[col].unique().tolist()]))\n        print('Unique in Test: ', sorted([str(e) for e in test[col].unique().tolist()]))\n        print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c50a37f6abe6ab3b456ca79118394805abcab60e"
      },
      "cell_type": "markdown",
      "source": "We'll merge the test and training dataset, and create a column which indicates whether a sample is in the test or train dataset.  That way, we can perform operations (label encoding, one-hot encoding, etc) to all the data together instead of doing it once to the training data and once to the test data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0346e412e98f1f7124a54b39903ea9057348e5e9"
      },
      "cell_type": "code",
      "source": "# Merge test and train into all application data\ntrain['Test'] = False\ntest['Test'] = True\ntest['TARGET'] = np.nan\napp = train.append(test, ignore_index=True)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n  sort=sort)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "17020939b123b2d49ac558c0ef14e6ef6da4bfb9"
      },
      "cell_type": "markdown",
      "source": "The gender column contains whether the loan applicant was male or female.  The training datset contains 4 values which weren't empty but were labelled `XNA`.  Normally we would want to create a new column to represent when the gender value is null.  However,  since the test dataset has only `M` and `F` entries, and because there are only 4 entries with a gender of `XNA` in the training set, we'll remove those entries from the training set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "734bd2ec150be7b5a1764f1bda3e0ced9dcd7e96"
      },
      "cell_type": "code",
      "source": "# Remove entries with gender = XNA\napp = app[app['CODE_GENDER'] != 'XNA']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88a9836bde9985a765da6194384abc5b76273371"
      },
      "cell_type": "markdown",
      "source": "The `NAME_INCOME_TYPE` column also contained entries for applicants who were on Maternity leave, but no such applicants were in the test set.  There were only 5 such applicants in the training set, so we'll remove these from the training set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa8f509cda7c020400a0c5718d6afe30bc17dd2c"
      },
      "cell_type": "code",
      "source": "# Remove entries with income type = maternity leave\napp = app[app['NAME_INCOME_TYPE'] != 'Maternity leave']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c249acebaa36a060854c7d624ef765342fc41366"
      },
      "cell_type": "markdown",
      "source": "Similarly, in the `NAME_FAMILY_STATUS` column, there were 2 entries in the training set with values of `Unknown`, and no entries with that value in the test set.  So, we'll remove those too."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "340b9ffbae4a9fe9762c12944f24e5bef62cf478"
      },
      "cell_type": "code",
      "source": "# Remove entries with unknown family status\napp = app[app['NAME_FAMILY_STATUS'] != 'Unknown']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1abcf81abdd12bac518b9ba6dbfae5549920d9b5"
      },
      "cell_type": "markdown",
      "source": "There were some funky values in the `DAYS_EMPLOYED` column:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c77e621099e7159f337fe6c498156658b3ab1ef3"
      },
      "cell_type": "code",
      "source": "app['DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED')\nplt.ylabel('Count')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "18f7109b6142ea10aba85c8b66f70b8c87ada27d"
      },
      "cell_type": "markdown",
      "source": "$350,000$ days?  That's like $1,000$ years!  Looks like all the reasonable values represent the number of days between when the applicant was employed and the date of the loan application.  The unreasonable values are all exactly $365,243$, so we'll set those to `NaN`."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88c253d19d78167320e5c5f6d802c73e8d88e74b"
      },
      "cell_type": "code",
      "source": "# Show distribution of reasonable values\napp.loc[app['DAYS_EMPLOYED']<200000, 'DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED (which are less than 200,000)')\nplt.ylabel('Count')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbc994f55797034195413a8220f94a6316e34839"
      },
      "cell_type": "code",
      "source": "# Show all unique outlier values\napp.loc[app['DAYS_EMPLOYED']>200000, 'DAYS_EMPLOYED'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3b65921e206aba230ddb2c63c6e1eeb52352bfd"
      },
      "cell_type": "code",
      "source": "# Set unreasonable values to nan\napp['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2a4a910575fac46d1e0318f5c64286249ba64b28"
      },
      "cell_type": "markdown",
      "source": "The previous application data file appeared to have the same problem in several columns, so let's replace those instances with NaN as well."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "56cb718c7c31afc6d4c430b1e6ce3d7a48ce95ed"
      },
      "cell_type": "code",
      "source": "# Columns to replace large value with nan\nproblem_cols = [\n    'DAYS_FIRST_DRAWING', \n    'DAYS_FIRST_DUE',\n    'DAYS_LAST_DUE_1ST_VERSION',\n    'DAYS_LAST_DUE',\n    'DAYS_TERMINATION'\n]\n\n# Replace 365243 w/ NaN for each of those columns\nfor col in problem_cols:\n    prev_app[col].replace(365243, np.nan, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f527d11e598260a8ccd37994b0359a43687fb95c"
      },
      "cell_type": "markdown",
      "source": "## Manual Feature Engineering\n\nWe'll add some features which may be informative as to how likely an applicant is to repay their loan:\n\n- The proportion of the applicant's life they have been employed.  If a 23-year-old has only been employed for 4 years, this is fine.  If a 50-year-old has only ever been employed for 4 years, they may have trouble repaying their loan.\n- The ratio of credit to income.  More income than credit will likely help an applicant be able to repay their loan.\n- The ratio of income to annuity.\n- The ratio of income to annuity scaled by age.\n- The ratio of credit to annuity.  If an applicant has a high level of credit relative to their annuity, they may have trouble repaying their loan.\n- The ratio of credit to annuity, scaled by age.  If a young person doesn't have much annuity this doesn't really mean they're less likely to repay their loan."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc307395a9461e1a3ade2bb98f799c5aef3bcc95"
      },
      "cell_type": "code",
      "source": "app['PROPORTION_LIFE_EMPLOYED'] = app['DAYS_EMPLOYED'] / app['DAYS_BIRTH']\napp['INCOME_TO_CREDIT_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_CREDIT'] \napp['INCOME_TO_ANNUITY_RATIO'] = app['AMT_INCOME_TOTAL'] / app['AMT_ANNUITY']\napp['INCOME_TO_ANNUITY_RATIO_BY_AGE'] = app['INCOME_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\napp['CREDIT_TO_ANNUITY_RATIO'] = app['AMT_CREDIT'] / app['AMT_ANNUITY']\napp['CREDIT_TO_ANNUITY_RATIO_BY_AGE'] = app['CREDIT_TO_ANNUITY_RATIO'] * app['DAYS_BIRTH']\napp['INCOME_TO_FAMILYSIZE_RATIO'] = app['AMT_INCOME_TOTAL'] / app['CNT_FAM_MEMBERS']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1494ed2198f37d78528546fb590bbba561fc8df2"
      },
      "cell_type": "markdown",
      "source": "## Feature Encoding\n\nSome columns are non-numerical and will have to be encoded to numeric types so that our predictive algorithm can handle them.  We'll encode cyclical variables (like day of the week) into 2 dimensions, encode features with only two possible classes by assigning them 0 or 1, and one-hot encode categorical features with more than two classes.\n\nThe column `WEEKDAY_APPR_PROCESS_START` contains categorical information corresponding to the day of the week.  We could encode these categories as the values 1-7, but this would imply that Sunday and Monday are more similar than, say Tuesday and Sunday.  We could also one-hot encode the column into 7 new columns, but that would create 7 additional dimensions.  Seeing as the week is cyclical, we'll encode this information into two dimensions by encoding them using polar coordinates.  That is, we'll represent the days of the week as a circle.  That way, we can encode the days of the week independently, but only add two dimensions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d86fc449a9ae5faa208bd35dc91cd509c5334a0"
      },
      "cell_type": "code",
      "source": "# Create map from categories to polar projection\nDOW_map = {\n    'MONDAY':    0,\n    'TUESDAY':   1,\n    'WEDNESDAY': 2,\n    'THURSDAY':  3,\n    'FRIDAY':    4,\n    'SATURDAY':  5,\n    'SUNDAY':    6,\n}\nDOW_map1 = {k: np.cos(2*np.pi*v/7.0) for k, v in DOW_map.items()}\nDOW_map2 = {k: np.sin(2*np.pi*v/7.0) for k, v in DOW_map.items()}\n\n# Show encoding of days of week -> circle\ndays = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']\ntt = np.linspace(0, 2*np.pi, 200)\nxx = np.cos(tt)\nyy = np.sin(tt)\nplt.plot(xx,yy)\nplt.gca().axis('equal')\nplt.xlabel('Encoded Dimension 1')\nplt.ylabel('Encoded Dimension 2')\nplt.title('2D Projection of days of the week')\nfor day in days:\n    plt.text(DOW_map1[day], DOW_map2[day], day, ha='center')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ac9340cde811b82e637d51d57b3f2c3e6c07655"
      },
      "cell_type": "code",
      "source": "# WEEKDAY_APPR_PROCESS_START to polar coords\ncol = 'WEEKDAY_APPR_PROCESS_START'\napp[col+'_1'] = app[col].map(DOW_map1)\napp[col+'_2'] = app[col].map(DOW_map2)\napp.drop(columns=col, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08fa08c097ace5bda19092d7250fa445b89217fb"
      },
      "cell_type": "markdown",
      "source": "For the housing-related features (e.g. `LIVINGAPARTMENTS_MODE`, `BASEMENTAREA_AVG`, etc) there are combinations of some PREFIX (e.g. `LIVINGAPARTMENTS`,  `BASEMENTAREA`, etc) and some POSTFIX (e.g. `MODE`, `MEDI`, `AVG`, etc) into a variable `PREFIX_POSTFIX`.  However, if one value for a given PREFIX is empty, the other values for that PREFIX will also be empty.  \n\nFor each column which has some empty values, we want to add an indicator column which is 1 if the value in the corresponding column is empty, and 0 otherwise.  However, if we do this with the housing-related features, we'll end up with a bunch of duplicate columns!  This is because the same samples have null values across all the POSTFIX columns for a given PREFIX.   The same problem crops up with the CREDIT_BUREAU-related features. To handle this problem, after creating the null indicator columns, we'll check for duplicate columns and merge them.\n\nSo, first we'll add columns to indicate where there are empty values in each other column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02acbadff9f44a022fc008c8aeba9ba72b8bd553"
      },
      "cell_type": "code",
      "source": "# Add indicator columns for empty values\nfor col in app:\n    if col!='Test' and col!='TARGET':\n        app_null = app[col].isnull()\n        if app_null.sum()>0:\n            app[col+'_ISNULL'] = app_null",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d64ad0c6824827fae7a5a04fce45d7768360d71"
      },
      "cell_type": "markdown",
      "source": "Then we can label encode categorical features with only 2 possible values (that is, turn the labels into either 0 or 1)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0167a10c72607282e6c6722d829be3beb146cc06"
      },
      "cell_type": "code",
      "source": "# Label encoder\nle = LabelEncoder()\n\n# Label encode binary fearures in training set\nfor col in app: \n    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()==2:\n        if col+'_ISNULL' in app.columns: #missing values here?\n            app.loc[app[col+'_ISNULL'], col] = 'NaN'\n        app[col] = le.fit_transform(app[col])\n        if col+'_ISNULL' in app.columns: #re-remove missing vals\n            app.loc[app[col+'_ISNULL'], col] = np.nan",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e0627f1939645184f0abc88b39f13367d92264a5"
      },
      "cell_type": "markdown",
      "source": "Then we'll one-hot encode the categorical features which have more than 2 possible values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bff906ff8e92bbf846d43e59fcb0b01150c1545f"
      },
      "cell_type": "code",
      "source": "# Get categorical features to encode\ncat_features = []\nfor col in app: \n    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()>2:\n        cat_features.append(col)\n\n# One-hot encode categorical features in train set\napp = pd.get_dummies(app, columns=cat_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3d531e3fb93e4eddc167aba65efad839f68a6774"
      },
      "cell_type": "markdown",
      "source": "And finally we'll remove duplicate columns.  We'll hash the columns and check if the hashes match before checking if all the values actually match, because it's a lot faster than comparing $O(N^2)$ columns elementwise."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "9f97dbdb0e4ec6885e533d34891573d57b2b3a73"
      },
      "cell_type": "code",
      "source": "# Hash columns\nhashes = dict()\nfor col in app:\n    hashes[col] = sha256(app[col].values).hexdigest()\n    \n# Get list of duplicate column lists\nNcol = app.shape[1] #number of columns\ndup_list = []\ndup_labels = -np.ones(Ncol)\nfor i1 in range(Ncol):\n    if dup_labels[i1]<0: #if not already merged,\n        col1 = app.columns[i1]\n        t_dup = [] #list of duplicates matching col1\n        for i2 in range(i1+1, Ncol):\n            col2 = app.columns[i2]\n            if ( dup_labels[i2]<0 #not already merged\n                 and hashes[col1]==hashes[col2] #hashes match\n                 and app[col1].equals(app[col2])): #cols are equal\n                #then this is actually a duplicate\n                t_dup.append(col2)\n                dup_labels[i2] = i1\n        if len(t_dup)>0: #duplicates of col1 were found!\n            t_dup.append(col1)\n            dup_list.append(t_dup)\n        \n# Merge duplicate columns\nfor iM in range(len(dup_list)):\n    new_name = 'Merged'+str(iM)\n    app[new_name] = app[dup_list[iM][0]].copy()\n    app.drop(columns=dup_list[iM], inplace=True)\n    print('Merged', dup_list[iM], 'into', new_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "45a690fa11a3f40319fbfe0d11febf80bdb84202"
      },
      "cell_type": "markdown",
      "source": "## Automated Feature Engineering\n\nSo far we've been dealing with only the data in the main application data files (application_train.csv and application_test.csv).  However, there is more additional information available about each loan applicant in additional files.  These additional files contain information about previous applications, monthly balances and payment data on previous loans, monthly credit card balances, and data from the Credit Bureau about the applicant's loans at other institutinons.  Entries in these additional files are connected to entries the main application file by an application ID column, or the applicant's id at the Credit Bureau:\n\n![File connection columns](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n\nWe could manually go through all these databases and construct features based on them, but this would entail not just a lot of manual work, but a *lot* of design decisions.  For example, should we construct a feature which corresponds to the maximum amount of credit the applicant has ever carried?  Or the average amount of credit?  Or the monthly median credit?  Should we construct a feature for how many payments the applicant has made, or how regular their payments are, or *when* they make their payments, etc, etc, etc?  \n\n[Featuretools](https://www.featuretools.com/) is a package which allows us to define the relationships between our datasets (that is, how a row in a parent dataset maps to an id of a certain column in a dataset which is a child of the parent dataset), and automatically extracts features from child datasets into parent datsets using \"feature primitives\" like min, max, mean, count, time since, etc.  We'll use featuretools to generate features from the data in the secondary datasets, and keep features which are informative.\n\nThe first step in using Featuretools is to define the \"entities\", each of which is one data file or table, and the columns along which they are indexed."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec78afa14c429bc7b66ddccc7bd4ea6d0b993bd0"
      },
      "cell_type": "code",
      "source": "# Define entities\n# Each entry is \"Name\", (df, \"id_col_name\")\nentities = { #use id_col_name not in df for new index, w/ None uses 1st col\n    'app': (app, 'SK_ID_CURR'),\n    'bureau': (bureau, 'SK_ID_BUREAU'),\n    'bureau_balance': (bureau_balance, 'New'),\n    'cash_balance': (cash_balance, 'New'),\n    'card_balance': (card_balance, 'New'),\n    'prev_app': (prev_app, 'SK_ID_PREV'),\n    'payments': (payments, 'New') \n}",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "304e96216d96164e2c6cfa7d8268c41d2c59feb5"
      },
      "cell_type": "markdown",
      "source": "Next we'll define the relationships between these entities (how each row in one dataset relates to a row in another dataset, based of an ID value in a given column for each entity)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12c849c9a6770387d2f46be550ec11d4b84a0889"
      },
      "cell_type": "code",
      "source": "# Define relationships between dataframes\n# Each entry is (parent_entity, parent_variable, child_entity, child_variable)\nrelationships = [\n    ('app', 'SK_ID_CURR', 'bureau', 'SK_ID_CURR'),\n    ('bureau', 'SK_ID_BUREAU', 'bureau_balance', 'SK_ID_BUREAU'),\n    ('app', 'SK_ID_CURR', 'prev_app', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'cash_balance', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'payments', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'card_balance', 'SK_ID_CURR')\n]",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "544399c1ec67a471e3c82e0f3d3f653012e2e8cd"
      },
      "cell_type": "markdown",
      "source": "Next we'll define which \"feature primitives\" we want to use to construct features.  First let's look at a list of all the feature primitives available in Featuretools:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d9c1f6c022a6971e14e5873a41c3f781f8c6dcf"
      },
      "cell_type": "code",
      "source": "pd.options.display.max_rows = 100\nft.list_primitives()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de4f968135ce8eb2fef40a17dc52d4e67543324d"
      },
      "cell_type": "markdown",
      "source": "We'll use a simple set of feature primitives: just the mean and the count of entries in the secondary data files."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0709ebdc7d9a32ad377b4a4f9086885acfeccea9"
      },
      "cell_type": "code",
      "source": "# Define which primitives to use\nagg_primitives =  ['count', 'mean', 'num_unique', 'percent_true']\ntrans_primitives = ['time_since_previous']",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27ad8a5a61a66c57e694d84449567ecec504ac14"
      },
      "cell_type": "markdown",
      "source": "Finally, we can run deep feature synthesis on our entities given their relationships and a list of feature primitives."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6af0a95932e23095b68f317133f790b23b5a3de"
      },
      "cell_type": "code",
      "source": "# Run deep feature synthesis\nt0 = time.time()\ndfs_feat, dfs_defs = ft.dfs(entities=entities,\n                            relationships=relationships,\n                            target_entity='app',\n                            trans_primitives=trans_primitives,\n                            agg_primitives=agg_primitives, \n                            verbose = True,\n                            max_depth=2, n_jobs=2)\nprint('DFS took %0.3g sec' % (time.time()-t0))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2018-08-29 20:33:00,480 featuretools.entityset - WARNING    index New not found in dataframe, creating new integer column\n2018-08-29 20:33:00,492 featuretools.entityset - WARNING    index New not found in dataframe, creating new integer column\n2018-08-29 20:33:00,508 featuretools.entityset - WARNING    index New not found in dataframe, creating new integer column\n2018-08-29 20:33:00,610 featuretools.entityset - WARNING    index New not found in dataframe, creating new integer column\nBuilt 219 features\nEntitySet scattered to workers in 2.697 seconds\nElapsed: 00:06 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 11/11 chunks",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Future exception was never retrieved\nfuture: <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed',)>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 177, in read\n    n_frames = yield stream.read_bytes(8)\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1133, in run\n    value = future.result()\ntornado.iostream.StreamClosedError: Stream is closed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1141, in run\n    yielded = self.gen.throw(*exc_info)\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 198, in read\n    convert_stream_closed_error(self, e)\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\n    raise CommClosedError(\"in %s: %s\" % (obj, exc))\ndistributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "\nDFS took 39.9 sec\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "215c615d1945d94337cebd310ad62ba1ab36a379"
      },
      "cell_type": "code",
      "source": "# Delete things we don't need anymore\ngc.enable()\ndel train, test, app, bureau, bureau_balance, cash_balance, prev_app, payments\ngc.collect()\n\n# Use featuretools features\napp = dfs_feat",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Exception ignored in: <generator object add_client at 0x7faf663bed58>\nRuntimeError: generator ignored GeneratorExit\nFuture exception was never retrieved\nfuture: <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed',)>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 177, in read\n    n_frames = yield stream.read_bytes(8)\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1133, in run\n    value = future.result()\ntornado.iostream.StreamClosedError: Stream is closed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1141, in run\n    yielded = self.gen.throw(*exc_info)\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 198, in read\n    convert_stream_closed_error(self, e)\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\n    raise CommClosedError(\"in %s: %s\" % (obj, exc))\ndistributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed\nFuture exception was never retrieved\nfuture: <Future finished exception=CommClosedError('in <closed TCP>: Stream is closed',)>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 177, in read\n    n_frames = yield stream.read_bytes(8)\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1133, in run\n    value = future.result()\ntornado.iostream.StreamClosedError: Stream is closed\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 1141, in run\n    yielded = self.gen.throw(*exc_info)\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 198, in read\n    convert_stream_closed_error(self, e)\n  File \"/opt/conda/lib/python3.6/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\n    raise CommClosedError(\"in %s: %s\" % (obj, exc))\ndistributed.comm.core.CommClosedError: in <closed TCP>: Stream is closed\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4bf2213a228a45b6ed972ce7215d00a5dff2554f"
      },
      "cell_type": "markdown",
      "source": "If we take a look at the dataframe which was returned by Featuretools, we can see that a bunch of features were appended which correspond to our selected feature primitive functions applied to data in the secondary data files which correspond to each row in the main application dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d49855795788d3205c015f06fbd5523546f77fe7"
      },
      "cell_type": "code",
      "source": "app",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "449b155cf051ac44fc2bb76c1ff20a50483abd48"
      },
      "cell_type": "markdown",
      "source": "Now that we've generated a bunch of features, we should make sure to remove ones which don't carry any information.  Featuretools includes a function to remove features which are entirely NULLs or only have one class, etc:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40f8cc6111003c60f474e7fce758a9d8430c601c"
      },
      "cell_type": "code",
      "source": "# Remove low information features\nNf0 = app.shape[1] #number of initial features\napp = app.remove_low_information_features(dfs_feat)\nprint('Removed', Nf0-app.shape[1], 'features')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac970aaf98fba223fd890e7147a0ceefb9a79af7"
      },
      "cell_type": "markdown",
      "source": "## Baseline Predictions\n\nAs a baseline, let's use XGBoost with all the default parameters to predict the probabilities of applicants having trouble repaying their loans."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d06d46ac51599ae674c72da0484bbd29a46e2d96"
      },
      "cell_type": "code",
      "source": "# Split data back into test + train\ntrain = app.loc[~app['Test'], :]\ntest = app.loc[app['Test'], :]\n\n# Make SK_ID_CURR the index\ntrain.set_index('SK_ID_CURR', inplace=True)\ntest.set_index('SK_ID_CURR', inplace=True)\n\n# Ensure all data is stored as floats\ntrain = train.astype(np.float32)\ntest = test.astype(np.float32)\n\n# Target labels\ntrain_y = train['TARGET']\n\n# Remove test/train indicator column and target column\ntrain.drop(columns=['Test', 'TARGET'], inplace=True)\ntest.drop(columns=['Test', 'TARGET'], inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce24926c749f1eff08f211b8d585a3b6bcfdd89c"
      },
      "cell_type": "code",
      "source": "# Classification pipeline\nxgb_pipeline = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', XGBClassifier())\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d44370077a0fd685c671d5133e42245a7fbfdbdc"
      },
      "cell_type": "code",
      "source": "# Cross-validated AUROC\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\nscores = cross_val_score(xgb_pipeline, train, train_y, \n                         cv=3, scoring=auroc_scorer)\nprint('Mean AUROC:', scores.mean())\n\n# Fit to training data\nxgb_fit = xgb_pipeline.fit(train, train_y)\n\n# Predict default probabilities of test data\ntest_pred = xgb_fit.predict_proba(test)\n\n# Save predictions to file\ndf_out = pd.DataFrame()\ndf_out['SK_ID_CURR'] = test.index\ndf_out['TARGET'] = test_pred[:,1]\ndf_out.to_csv('xgboost_baseline.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c8dfeaa47cb8f1f1d09aa94235ed6a77787a18da"
      },
      "cell_type": "markdown",
      "source": "## Calibration\n\nOne problem with the tree-based model is that the predicted probabilities tend to be overconfident.  That is, when the actual probability of class=1 is closer to 0.5, the model predicts probabilities closer to 0 or 1 than 0.5.  We can measure the extent of this overconfidence (or underconfidence) of our classifier by looking at its calibration curve.  The calibration curve plots the probability predicted by our model against the actual probability of samples in that bin.  A model which is perfectly calibrated should show a calibration curve which lies on the identity (y=x) line."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "131cb3ca0f16c4709b25ca09429d75b00c5569e1"
      },
      "cell_type": "code",
      "source": "# Predict probabilities for training data\ntrain_pred = cross_val_predict(xgb_pipeline, \n                               train, \n                               y=train_y,\n                               method='predict_proba')\ntrain_pred = train_pred[:,1] #only want p(default)\n\n# Show calibration curve\nfraction_of_positives, mean_predicted_value = \\\n    calibration_curve(train_y, train_pred, n_bins=10)\nplt.figure()\nplt.plot([0, 1], [0, 1], 'k:', \n         label='Perfectly Calibrated')\nplt.plot(mean_predicted_value, \n         fraction_of_positives, 's-',\n         label='XGBoost Predictions')\nplt.legend()\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration curve for baseline XGBoost model')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a91eeb0cc697aa35f96e779ca53ab603d8681c6"
      },
      "cell_type": "markdown",
      "source": "The model is pretty well calibrated as is, exept for at higher predicted probabilities.  We can better calibrate our model by adjusting predicted probabilities to more accurately reflect the probability of loan default.  \n\nThere are two commonly-used methods for model calibration:\n\n1. Sigmoid calibration (aka Platt's scaling, which transforms the model's predictions using a sigmoid so they more accurately reflect the actual probabilities)\n1. Isotonic calibration (which calibrates the model's predictions using a method based on isotonic regression)\n\nWe'll try both methods, and see if either betters the calibration of our model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b19260e3e782c46d25349acc700b95d50b8eaf3"
      },
      "cell_type": "code",
      "source": "# Classification pipeline w/ isotonic calibration\ncalib_pipeline = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', CalibratedClassifierCV(\n                        base_estimator=XGBClassifier(),\n                        method='isotonic'))\n])\n\n# Classification pipeline w/ sigmoid calibration\nsig_pipeline = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', CalibratedClassifierCV(\n                        base_estimator=XGBClassifier(),\n                        method='sigmoid'))\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea1cf2f6cf67d8f4ac02d47b63407b1e38c2d0d7"
      },
      "cell_type": "code",
      "source": "# Predict probabilities w/ isotonic calibration\ncalib_pred = cross_val_predict(calib_pipeline, \n                               train, \n                               y=train_y,\n                               method='predict_proba')\ncalib_pred = calib_pred[:,1] #only want p(default)\n\n# Predict probabilities w/ sigmoid calibration\nsig_pred = cross_val_predict(sig_pipeline, \n                             train, \n                             y=train_y,\n                             method='predict_proba')\nsig_pred = sig_pred[:,1] #only want p(default)\n\n# Show calibration curve\nfop_calib, mpv_calib = \\\n    calibration_curve(train_y, calib_pred, n_bins=10)\nfop_sig, mpv_sig = \\\n    calibration_curve(train_y, sig_pred, n_bins=10)\nplt.figure()\nplt.plot([0, 1], [0, 1], 'k:', \n         label='Perfectly Calibrated')\nplt.plot(mean_predicted_value, \n         fraction_of_positives, 's-',\n         label='XGBoost Predictions')\nplt.plot(mpv_calib, fop_calib, 's-',\n         label='Calibrated Predictions - isotonic')\nplt.plot(mpv_sig, fop_sig, 's-',\n         label='Calibrated Predictions - sigmoid')\nplt.legend()\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration curve for Calibrated XGBoost model')\nplt.show()\n\n# Cross-validated AUROC for isotonic\nprint('Mean AUROC with isotonic calibration:', \n      roc_auc_score(train_y, calib_pred))\n\n# Cross-validated AUROC for sigmoid\nprint('Mean AUROC with sigmoid calibration:',\n      roc_auc_score(train_y, sig_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c5fc4cc8555d7411e5cb5e35ce93e9c86e56a637"
      },
      "cell_type": "markdown",
      "source": "Sigmoid calibration didn't appear to work very well in this case...  Isotonic calibration didn't work perfectly either, however it did appear to improve the model's discrimination a small bit (the model without calibration has slightly poorer discrimination in that it is more likely to predict probabilities which are close to 0.5).  Isotonic calibration is usually only recommended if one has $>>1000$ datapoints, which we do (the training set contains around 300,000 datapoins), so we'll go ahead and use isotonic calibration.  Now we can output our predictions after calibrating."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2e32d52ae44dd8a8999348efc46b51a51c47727"
      },
      "cell_type": "code",
      "source": "# Fit to training data\ncalib_fit = calib_pipeline.fit(train, train_y)\n\n# Predict default probabilities of test data\ntest_pred = calib_fit.predict_proba(test)\n\n# Save predictions to file\ndf_out = pd.DataFrame()\ndf_out['SK_ID_CURR'] = test.index\ndf_out['TARGET'] = test_pred[:,1]\ndf_out.to_csv('xgboost_calibrated.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "46c0e74b6db206fee84203dbb9692ff5972b6fdc"
      },
      "cell_type": "markdown",
      "source": "## Resampling to handle the Class Imbalance\n\nThe target class is very imbalanced: many more people successfully repaid their loans than had trouble repaying."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c3b18085c1a7de46051f4db32047b18f48b6ef0"
      },
      "cell_type": "code",
      "source": "# Show distribution of target variable\nsns.countplot(x='TARGET', data=app)\nplt.title('Number of applicants who had trouble repaying')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58811a46c0572302841db86e72147a7066929741"
      },
      "cell_type": "markdown",
      "source": "We'll use the [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html) package to re-sample our dataset such that the classes are balanced.  There are several different common methods we could use for re-sampling: \n\n1. Random over-sampling (randomly repeat minority class examples in the training data)\n1. Random under-sampling (randomly drop majority class examples from the training data)\n1. Synthetic minority oversampling technique (SMOTE, generate additional synthetic training examples which are similar to the minority class)\n\nWe'll try all three techniques, and see if any of the techniques give better predictive performance in terms of the AUROC."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e95b3c42f930e5fd8375242cc6108726550e5f80"
      },
      "cell_type": "code",
      "source": "# Sampler that doesn't re-sample!\nclass DummySampler(object):\n    def sample(self, X, y):\n        return X, y\n    def fit(self, X, y):\n        return self\n    def fit_sample(self, X, y):\n        return self.sample(X, y)\n    \n# List of samplers to test\nsamplers = [\n    ['Oversampling', RandomOverSampler()], \n    ['Undersampling', RandomUnderSampler()], \n    ['SMOTE', SMOTE()],\n    ['No resampling', DummySampler()]\n]\n\n# Preprocessing pipeline\npre_pipeline = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('imputer', SimpleImputer(strategy='median'))\n])\n\n# Classifier\nclassifier = CalibratedClassifierCV(\n                        base_estimator=XGBClassifier(),\n                        method='isotonic')\n\n# Compute AUROC and plot ROC for each type of sampler\nplt.figure()\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\ncv = StratifiedKFold(n_splits=3)\nfor name, sampler in samplers:\n    \n    # Make the sampling and classification pipeline\n    pipeline = make_pipeline(sampler, calib_pipeline)\n\n    # Cross-validated predictions on training set\n    probas = np.zeros(train.shape[0]) # to store predicted probabilities\n    for tr, te in cv.split(train, train_y):\n        test_pre = pre_pipeline.fit_transform(train.iloc[te])  #preprocess test fold\n        train_pre = pre_pipeline.fit_transform(train.iloc[tr]) #preprocess training fold\n        train_s, train_y_s = sampler.fit_sample(train_pre, train_y.iloc[tr]) #resample train fold\n        probas_ = classifier.fit(train_s, train_y_s).predict_proba(test_pre) #predict test fold\n        probas[te] = probas_[:,1]\n    \n    # Print AUROC value\n    print(name, 'AUROC:', roc_auc_score(train_y, probas))\n    \n    # Plot ROC curve for this sampler\n    fpr, tpr, threshs = roc_curve(train_y, probas)\n    plt.plot(fpr, tpr, label=name)\n\nplt.plot([0, 1], [0, 1], label='Chance')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "826e600e54c5fadc7cd65b89f36ee28d61a958eb"
      },
      "cell_type": "markdown",
      "source": "Unfortunately it looks like none of the sampling techniques actually helped improve the AUROC score!  The SMOTE resampling technique did even more poorly than simply under- or over-sampling.  This is probably because SMOTE generates samples by interpolating between training samples in feature-space, but most of our features are binary.  So, interpolation isn't really adding diversity to the training data, it's just adding noise and making it more difficult for our classification algorithm to decide where to put a threshold in that dimension."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80aaa3097ea3b19dfab54a774d426b7e3e54840a"
      },
      "cell_type": "code",
      "source": "from lightgbm import LGBMClassifier\n\n# The classifier object\nLGBMC = LGBMClassifier(\n    n_estimators=10000,\n    learning_rate=0.03,\n    num_leaves=35,\n    colsample_bytree=0.95,\n    subsample=0.85,\n    max_depth=8,\n    reg_alpha=0.04,\n    reg_lambda=0.07,\n    min_split_gain=0.02,\n    min_child_weight=35,\n)\n\n# Classification pipeline w/ lightGBM instead of Xgboost\nlgbm_pipeline = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', CalibratedClassifierCV(\n                        base_estimator=LGBMC,\n                        method='isotonic'))\n])\n\n# Cross-validated AUROC\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\nscores = cross_val_score(lgbm_pipeline, train, train_y, \n                         cv=3, scoring=auroc_scorer)\nprint('Mean AUROC for LightGBM:', scores.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5225207564540f0f72f89efb54c6da209826b7d6"
      },
      "cell_type": "markdown",
      "source": "\n## Final Predictions and Feature Importance\n\nTODO:\n\n- talk about how you'll use the isotonic calibrated model, and no resampling\n- Feature importance plots\n- Talk about monitoring and correcting for systematic racism, etc\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}