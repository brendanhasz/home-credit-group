{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom xgboost import XGBClassifier\nfrom hashlib import sha256\nimport time\nimport featuretools as ft\nsns.set()",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5402661119519f4267bfbd3d53713ed2c1aa73bb"
      },
      "cell_type": "markdown",
      "source": "## Data Loading and Cleaning"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbafb8dcf1af4e7d9c3b4bf16161327a6ae695dd"
      },
      "cell_type": "code",
      "source": "# Load applications data\ntrain = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\n#bureau = pd.read_csv('../input/bureau.csv')\n#bureau_balance = pd.read_csv('../input/bureau_balance.csv')\n#cash_balance = pd.read_csv('../input/POS_CASH_balance.csv')\n#card_balance = pd.read_csv('../input/credit_card_balance.csv')\n#prev_app = pd.read_csv('../input/previous_application.csv')\n#payments = pd.read_csv('../input/installments_payments.csv')",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "569b63d0d9d83e59de73fb8dbcbf98c70a51ad7f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "c83bc5b0363e361864d48367e1b416fa37254300",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Print info about each column in the train dataset\nfor col in train:\n    print(col)\n    Nnan = train[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/train.shape[0])\n    print(train[col].describe())\n    if train[col].dtype==object:\n        print('Categories and Count:')\n        print(train[col].value_counts().to_string(header=None))\n    print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da59786e9376f69b3b179ba596428f12818a5ad4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Print info about each column in the test dataset\nfor col in test:\n    print(col)\n    Nnan = test[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/test.shape[0])\n    print(test[col].describe())\n    if test[col].dtype==object:\n        print('Categories and Count:')\n        print(test[col].value_counts().to_string(header=None))\n    print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "33f2e6988f3fb8f05023b55a6a9d74e47bc67958"
      },
      "cell_type": "markdown",
      "source": "The column containing the values we are trying to predict, `TARGET`, doesn't contain any missing values.  The value of `TARGET` is $0$ when the loan was repayed sucessfully, and $1$ when there were problems repaying the loan.  Many more loans were succesfully repayed than not, which means that the dataset is imbalanced in terms of our dependent variable, which is something we'll have to watch out for when we build a predictive model later:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d73aae2ef303cdce678f87bc49c164073f26b081",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Show target distribution\ntrain['TARGET'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae4313940302e607227540defc3302c6a32d856c"
      },
      "cell_type": "markdown",
      "source": "There's a lot of categorical columns - let's check that, for each column, all the categories we see in the training set we also see in the test set, and vice-versa."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc7177f28a14c85162c608d9e85bee10ffe432a4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for col in test:\n    if test[col].dtype==object:\n        print(col)\n        print('Num Unique in Train:', train[col].nunique())\n        print('Num Unique in Test: ', test[col].nunique())\n        print('Unique in Train:', sorted([str(e) for e in train[col].unique().tolist()]))\n        print('Unique in Test: ', sorted([str(e) for e in test[col].unique().tolist()]))\n        print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c50a37f6abe6ab3b456ca79118394805abcab60e"
      },
      "cell_type": "markdown",
      "source": "We'll merge the test and training dataset, and create a column which indicates whether a sample is in the test or train dataset.  That way, we can perform operations (label encoding, one-hot encoding, etc) to all the data together instead of doing it once to the training data and once to the test data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0346e412e98f1f7124a54b39903ea9057348e5e9"
      },
      "cell_type": "code",
      "source": "# Merge test and train into all application data\ntrain['Test'] = False\ntest['Test'] = True\ntest['TARGET'] = np.nan\napp = train.append(test, ignore_index=True)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n  sort=sort)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "17020939b123b2d49ac558c0ef14e6ef6da4bfb9"
      },
      "cell_type": "markdown",
      "source": "The gender column contains whether the loan applicant was male or female.  The training datset contains 4 values which weren't empty but were labelled `XNA`.  Normally we would want to create a new column to represent when the gender value is null.  However,  since the test dataset has only `M` and `F` entries, and because there are only 4 entries with a gender of `XNA` in the training set, we'll remove those entries from the training set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "734bd2ec150be7b5a1764f1bda3e0ced9dcd7e96"
      },
      "cell_type": "code",
      "source": "# Remove entries with gender = XNA\napp = app[app['CODE_GENDER'] != 'XNA']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "88a9836bde9985a765da6194384abc5b76273371"
      },
      "cell_type": "markdown",
      "source": "The `NAME_INCOME_TYPE` column also contained entries for applicants who were on Maternity leave, but no such applicants were in the test set.  There were only 5 such applicants in the training set, so we'll remove these from the training set."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa8f509cda7c020400a0c5718d6afe30bc17dd2c"
      },
      "cell_type": "code",
      "source": "# Remove entries with income type = maternity leave\napp = app[app['NAME_INCOME_TYPE'] != 'Maternity leave']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c249acebaa36a060854c7d624ef765342fc41366"
      },
      "cell_type": "markdown",
      "source": "Similarly, in the `NAME_FAMILY_STATUS` column, there were 2 entries in the training set with values of `Unknown`, and no entries with that value in the test set.  So, we'll remove those too."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "340b9ffbae4a9fe9762c12944f24e5bef62cf478"
      },
      "cell_type": "code",
      "source": "# Remove entries with unknown family status\napp = app[app['NAME_FAMILY_STATUS'] != 'Unknown']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1abcf81abdd12bac518b9ba6dbfae5549920d9b5"
      },
      "cell_type": "markdown",
      "source": "There were some funky values in the `DAYS_EMPLOYED` column:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c77e621099e7159f337fe6c498156658b3ab1ef3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "app['DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED')\nplt.ylabel('Count')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "18f7109b6142ea10aba85c8b66f70b8c87ada27d"
      },
      "cell_type": "markdown",
      "source": "$350,000$ days?  That's like $1,000$ years!  Looks like all the reasonable values represent the number of days between when the applicant was employed and the date of the loan application.  The unreasonable values are all exactly $365,243$, so we'll set those to `NaN`."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88c253d19d78167320e5c5f6d802c73e8d88e74b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Show distribution of reasonable values\napp.loc[app['DAYS_EMPLOYED']<200000, 'DAYS_EMPLOYED'].hist()\nplt.xlabel('DAYS_EMPLOYED (which are less than 200,000)')\nplt.ylabel('Count')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbc994f55797034195413a8220f94a6316e34839",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Show all unique outlier values\napp.loc[app['DAYS_EMPLOYED']>200000, 'DAYS_EMPLOYED'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3b65921e206aba230ddb2c63c6e1eeb52352bfd"
      },
      "cell_type": "code",
      "source": "# Set unreasonable values to nan\napp.loc[app['DAYS_EMPLOYED']==365243, 'DAYS_EMPLOYED'] = np.nan",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1494ed2198f37d78528546fb590bbba561fc8df2"
      },
      "cell_type": "markdown",
      "source": "The column `WEEKDAY_APPR_PROCESS_START` contains categorical information corresponding to the day of the week.  We could encode these categories as the values 1-7, but this would imply that Sunday and Monday are more similar than, say Tuesday and Sunday.  We could also one-hot encode the column into 7 new columns, but that would create 7 additional dimensions.  Seeing as the week is cyclical, we'll encode this information into two dimensions by encoding them using polar coordinates.  That is, we'll represent the days of the week as a circle.  That way, we can encode the days of the week independently, but only add two dimensions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d86fc449a9ae5faa208bd35dc91cd509c5334a0"
      },
      "cell_type": "code",
      "source": "# Create map from categories to polar projection\nDOW_map = {\n    'MONDAY':    0,\n    'TUESDAY':   1,\n    'WEDNESDAY': 2,\n    'THURSDAY':  3,\n    'FRIDAY':    4,\n    'SATURDAY':  5,\n    'SUNDAY':    6,\n}\nDOW_map1 = {k: np.cos(2*np.pi*v/7.0) for k, v in DOW_map.items()}\nDOW_map2 = {k: np.sin(2*np.pi*v/7.0) for k, v in DOW_map.items()}\n\n# Show encoding of days of week -> circle\ndays = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']\ntt = np.linspace(0, 2*np.pi, 200)\nxx = np.cos(tt)\nyy = np.sin(tt)\nplt.plot(xx,yy)\nplt.gca().axis('equal')\nplt.xlabel('Encoded Dimension 1')\nplt.ylabel('Encoded Dimension 2')\nplt.title('2D Projection of days of the week')\nfor day in days:\n    plt.text(DOW_map1[day], DOW_map2[day], day, ha='center')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ac9340cde811b82e637d51d57b3f2c3e6c07655"
      },
      "cell_type": "code",
      "source": "# WEEKDAY_APPR_PROCESS_START to polar coords\ncol = 'WEEKDAY_APPR_PROCESS_START'\napp[col+'_1'] = app[col].map(DOW_map1)\napp[col+'_2'] = app[col].map(DOW_map2)\napp.drop(columns=col, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08fa08c097ace5bda19092d7250fa445b89217fb"
      },
      "cell_type": "markdown",
      "source": "For the housing-related features (e.g. `LIVINGAPARTMENTS_MODE`, `BASEMENTAREA_AVG`, etc) there are combinations of some PREFIX (e.g. `LIVINGAPARTMENTS`,  `BASEMENTAREA`, etc) and some POSTFIX (e.g. `MODE`, `MEDI`, `AVG`, etc) into a variable `PREFIX_POSTFIX`.  However, if one value for a given PREFIX is empty, the other values for that PREFIX will also be empty.  \n\nFor each column which has some empty values, we want to add an indicator column which is 1 if the value in the corresponding column is empty, and 0 otherwise.  However, if we do this with the housing-related features, we'll end up with a bunch of duplicate columns!  This is because the same samples have null values across all the POSTFIX columns for a given PREFIX.   The same problem crops up with the CREDIT_BUREAU-related features. To handle this problem, after creating the null indicator columns, we'll check for duplicate columns and merge them.\n\nSo, first we'll add columns to indicate where there are empty values in each other column."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02acbadff9f44a022fc008c8aeba9ba72b8bd553"
      },
      "cell_type": "code",
      "source": "# Add indicator columns for empty values\nfor col in app:\n    app_null = app[col].isnull()\n    if app_null.sum()>0:\n        app[col+'_ISNULL'] = app_null",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d64ad0c6824827fae7a5a04fce45d7768360d71"
      },
      "cell_type": "markdown",
      "source": "Then we can label encode categorical features with only 2 possible values (that is, turn the labels into either 0 or 1)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f484fc09efb73a9eea38fc0343be80137860116"
      },
      "cell_type": "code",
      "source": "for col in app:\n    if col!='Test' and col!='TARGET':\n        print(col)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "AMT_ANNUITY\nAMT_CREDIT\nAMT_GOODS_PRICE\nAMT_INCOME_TOTAL\nAMT_REQ_CREDIT_BUREAU_DAY\nAMT_REQ_CREDIT_BUREAU_HOUR\nAMT_REQ_CREDIT_BUREAU_MON\nAMT_REQ_CREDIT_BUREAU_QRT\nAMT_REQ_CREDIT_BUREAU_WEEK\nAMT_REQ_CREDIT_BUREAU_YEAR\nAPARTMENTS_AVG\nAPARTMENTS_MEDI\nAPARTMENTS_MODE\nBASEMENTAREA_AVG\nBASEMENTAREA_MEDI\nBASEMENTAREA_MODE\nCNT_CHILDREN\nCNT_FAM_MEMBERS\nCODE_GENDER\nCOMMONAREA_AVG\nCOMMONAREA_MEDI\nCOMMONAREA_MODE\nDAYS_BIRTH\nDAYS_EMPLOYED\nDAYS_ID_PUBLISH\nDAYS_LAST_PHONE_CHANGE\nDAYS_REGISTRATION\nDEF_30_CNT_SOCIAL_CIRCLE\nDEF_60_CNT_SOCIAL_CIRCLE\nELEVATORS_AVG\nELEVATORS_MEDI\nELEVATORS_MODE\nEMERGENCYSTATE_MODE\nENTRANCES_AVG\nENTRANCES_MEDI\nENTRANCES_MODE\nEXT_SOURCE_1\nEXT_SOURCE_2\nEXT_SOURCE_3\nFLAG_CONT_MOBILE\nFLAG_DOCUMENT_10\nFLAG_DOCUMENT_11\nFLAG_DOCUMENT_12\nFLAG_DOCUMENT_13\nFLAG_DOCUMENT_14\nFLAG_DOCUMENT_15\nFLAG_DOCUMENT_16\nFLAG_DOCUMENT_17\nFLAG_DOCUMENT_18\nFLAG_DOCUMENT_19\nFLAG_DOCUMENT_2\nFLAG_DOCUMENT_20\nFLAG_DOCUMENT_21\nFLAG_DOCUMENT_3\nFLAG_DOCUMENT_4\nFLAG_DOCUMENT_5\nFLAG_DOCUMENT_6\nFLAG_DOCUMENT_7\nFLAG_DOCUMENT_8\nFLAG_DOCUMENT_9\nFLAG_EMAIL\nFLAG_EMP_PHONE\nFLAG_MOBIL\nFLAG_OWN_CAR\nFLAG_OWN_REALTY\nFLAG_PHONE\nFLAG_WORK_PHONE\nFLOORSMAX_AVG\nFLOORSMAX_MEDI\nFLOORSMAX_MODE\nFLOORSMIN_AVG\nFLOORSMIN_MEDI\nFLOORSMIN_MODE\nFONDKAPREMONT_MODE\nHOUR_APPR_PROCESS_START\nHOUSETYPE_MODE\nLANDAREA_AVG\nLANDAREA_MEDI\nLANDAREA_MODE\nLIVE_CITY_NOT_WORK_CITY\nLIVE_REGION_NOT_WORK_REGION\nLIVINGAPARTMENTS_AVG\nLIVINGAPARTMENTS_MEDI\nLIVINGAPARTMENTS_MODE\nLIVINGAREA_AVG\nLIVINGAREA_MEDI\nLIVINGAREA_MODE\nNAME_CONTRACT_TYPE\nNAME_EDUCATION_TYPE\nNAME_FAMILY_STATUS\nNAME_HOUSING_TYPE\nNAME_INCOME_TYPE\nNAME_TYPE_SUITE\nNONLIVINGAPARTMENTS_AVG\nNONLIVINGAPARTMENTS_MEDI\nNONLIVINGAPARTMENTS_MODE\nNONLIVINGAREA_AVG\nNONLIVINGAREA_MEDI\nNONLIVINGAREA_MODE\nOBS_30_CNT_SOCIAL_CIRCLE\nOBS_60_CNT_SOCIAL_CIRCLE\nOCCUPATION_TYPE\nORGANIZATION_TYPE\nOWN_CAR_AGE\nREGION_POPULATION_RELATIVE\nREGION_RATING_CLIENT\nREGION_RATING_CLIENT_W_CITY\nREG_CITY_NOT_LIVE_CITY\nREG_CITY_NOT_WORK_CITY\nREG_REGION_NOT_LIVE_REGION\nREG_REGION_NOT_WORK_REGION\nSK_ID_CURR\nTOTALAREA_MODE\nWALLSMATERIAL_MODE\nWEEKDAY_APPR_PROCESS_START\nYEARS_BEGINEXPLUATATION_AVG\nYEARS_BEGINEXPLUATATION_MEDI\nYEARS_BEGINEXPLUATATION_MODE\nYEARS_BUILD_AVG\nYEARS_BUILD_MEDI\nYEARS_BUILD_MODE\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0167a10c72607282e6c6722d829be3beb146cc06"
      },
      "cell_type": "code",
      "source": "# Label encoder\nle = LabelEncoder()\n\n# Label encode binary fearures in training set\nfor col in app: \n    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()==2:\n        if col+'_ISNULL' in app.columns: #missing values here?\n            app.loc[app[col+'_ISNULL'], col] = 'NaN'\n        app[col] = le.fit_transform(app[col])\n        if col+'_ISNULL' in app.columns: #re-remove missing vals\n            app.loc[app[col+'_ISNULL'], col] = np.nan",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e0627f1939645184f0abc88b39f13367d92264a5"
      },
      "cell_type": "markdown",
      "source": "Then we'll one-hot encode the categorical features which have more than 2 possible values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bff906ff8e92bbf846d43e59fcb0b01150c1545f"
      },
      "cell_type": "code",
      "source": "# Get categorical features to encode\ncat_features = []\nfor col in app: \n    if col!='Test' and col!='TARGET' and app[col].dtype==object and app[col].nunique()>2:\n        cat_features.append(col)\n\n# One-hot encode categorical features in train set\napp = pd.get_dummies(app, columns=cat_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3d531e3fb93e4eddc167aba65efad839f68a6774"
      },
      "cell_type": "markdown",
      "source": "And finally we'll remove duplicate columns.  We'll hash the columns and check if the hashes match before checking if all the values actually match, because it's a lot faster than comparing $O(N^2)$ columns elementwise."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "9f97dbdb0e4ec6885e533d34891573d57b2b3a73"
      },
      "cell_type": "code",
      "source": "# Hash columns\nhashes = dict()\nfor col in app:\n    hashes[col] = sha256(app[col].values).hexdigest()\n    \n# Get list of duplicate column lists\nNcol = app.shape[1] #number of columns\ndup_list = []\ndup_labels = -np.ones(Ncol)\nfor i1 in range(Ncol):\n    if dup_labels[i1]<0: #if not already merged,\n        col1 = app.columns[i1]\n        t_dup = [] #list of duplicates matching col1\n        for i2 in range(i1+1, Ncol):\n            col2 = app.columns[i2]\n            if ( dup_labels[i2]<0 #not already merged\n                 and hashes[col1]==hashes[col2] #hashes match\n                 and app[col1].equals(app[col2])): #cols are equal\n                #then this is actually a duplicate\n                t_dup.append(col2)\n                dup_labels[i2] = i1\n        if len(t_dup)>0: #duplicates of col1 were found!\n            t_dup.append(col1)\n            dup_list.append(t_dup)\n        \n# Merge duplicate columns\nfor iM in range(len(dup_list)):\n    new_name = 'Merged'+str(iM)\n    app[new_name] = app[dup_list[iM][0]].copy()\n    app.drop(columns=dup_list[iM], inplace=True)\n    print('Merged', dup_list[iM], 'into', new_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "45a690fa11a3f40319fbfe0d11febf80bdb84202"
      },
      "cell_type": "markdown",
      "source": "## Deep Feature Synthesis\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec78afa14c429bc7b66ddccc7bd4ea6d0b993bd0"
      },
      "cell_type": "code",
      "source": "# Create entities\n# Each entry is \"Name\": (df, \"id_col_name\")\nentities = {\n    'app' : (app, 'SK_ID_CURR'),\n    'bureau' : (bureau, 'SK_ID_BUREAU'),\n    'bureau_balance' : (bureau, 'SK_ID_BUREAU'), #or None?\n    'cash_balance' : (cash_balance, 'SK_ID_CURR'), #or None?\n    'card_balance' : (card_balance, 'SK_ID_CURR'), #or None?\n    'prev_app' : (prev_app, 'SK_ID_PREV'),\n    'payments' : (payments, 'SK_ID_CURR') #or None?\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "304e96216d96164e2c6cfa7d8268c41d2c59feb5"
      },
      "cell_type": "markdown",
      "source": "NOTE: dunno if the above will work - some dfs (bureau_balance, cash_balance, card_balance, and payments) don't have indexes already.  Will it work as is?  Might have to set the id_col_name to None?  Or might have to do it the way as in the kaggle kernel where you call entity_from_dataframe with make_index=True"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12c849c9a6770387d2f46be550ec11d4b84a0889"
      },
      "cell_type": "code",
      "source": "# Define relationships between dataframes\n# Each entry is (parent_entity, parent_variable, child_entity, child_variable)\nrelationships = [\n    ('app', 'SK_ID_CURR', 'bureau', 'SK_ID_CURR'),\n    ('bureau', 'SK_ID_BUREAU', 'bureau_balance', 'SK_ID_BUREAU'),\n    ('app', 'SK_ID_CURR', 'prev_app', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'cash_balance', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'payments', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'card_balance', 'SK_ID_CURR')\n]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "544399c1ec67a471e3c82e0f3d3f653012e2e8cd"
      },
      "cell_type": "markdown",
      "source": "NOTE: cash_balance, payments, and card_balance could probs be linked to prev_app via SK_ID_PREV, instead of directly thru SK_ID_CURR, because they correspond to stuff from previous loans..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0709ebdc7d9a32ad377b4a4f9086885acfeccea9"
      },
      "cell_type": "code",
      "source": "# Define which primitives to use\nagg_primitives =  ['mean', 'std', 'min', 'max', 'count', 'percent_true', 'num_unique']\ntrans_primitives = [] #don't use any for now...\n\n# Run deep feature synthesis\nt0 = time.time()\ndfs_feat, dfs_defs = ft.dfs(entities=entities,\n                            relationships=relationships,\n                            target_entity='app',\n                            trans_primitives = default_trans_primitives,\n                            agg_primitives=default_agg_primitives, \n                            max_depth=1, features_only=True)\nprint('DFS took %0.3g sec' % (time.time()-t0))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bf2213a228a45b6ed972ce7215d00a5dff2554f"
      },
      "cell_type": "markdown",
      "source": "NOTE: this is just to see if DFS will work in a kaggle kernel. You'll have to then insert the dfs_feat matrix (deep feature synthesized feature matrix) back into the app dataframe (or replace the app dataframe with it, really)"
    },
    {
      "metadata": {
        "_uuid": "ac970aaf98fba223fd890e7147a0ceefb9a79af7"
      },
      "cell_type": "markdown",
      "source": "## Prediction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d06d46ac51599ae674c72da0484bbd29a46e2d96"
      },
      "cell_type": "code",
      "source": "# Split data back into test + train\ntrain = app.loc[~app['Test'], :]\ntest = app.loc[app['Test'], :]\n\n# Make SK_ID_CURR the index\ntrain.set_index('SK_ID_CURR', inplace=True)\ntest.set_index('SK_ID_CURR', inplace=True)\n\n# Ensure all data is stored as floats\ntrain = train.astype(np.float32)\ntest = test.astype(np.float32)\n\n# Target labels\ntrain_y = train['TARGET']\n\n# Remove test/train indicator column and target column\ntrain.drop(columns=['Test', 'TARGET'], inplace=True)\ntest.drop(columns=['Test', 'TARGET'], inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce24926c749f1eff08f211b8d585a3b6bcfdd89c"
      },
      "cell_type": "code",
      "source": "# Classification pipeline\nxgb_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', XGBClassifier())\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d44370077a0fd685c671d5133e42245a7fbfdbdc"
      },
      "cell_type": "code",
      "source": "# Cross-validated AUROC\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\nscores = cross_val_score(xgb_pipeline, train, train_y, \n                         cv=3, scoring=auroc_scorer)\nprint('Mean AUROC:', scores.mean())\n\n# Fit to training data\nxgb_fit = xgb_pipeline.fit(train, train_y)\n\n# Predict default probabilities of test data\ntest_pred = xgb_fit.predict_proba(test)\n\n# Save predictions to file\ndf_out = pd.DataFrame()\ndf_out['SK_ID_CURR'] = test.index\ndf_out['TARGET'] = test_pred[:,1]\ndf_out.to_csv('xgboost_baseline.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c8dfeaa47cb8f1f1d09aa94235ed6a77787a18da"
      },
      "cell_type": "markdown",
      "source": "## Calibration\n\nOne problem with the tree-based model is that the predicted probabilities tend to be overconfident.  That is, when the actual probability of class=1 is closer to 0.5, the model predicts probabilities closer to 0 or 1 than 0.5:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "131cb3ca0f16c4709b25ca09429d75b00c5569e1"
      },
      "cell_type": "code",
      "source": "# Predict probabilities for training data\ntrain_pred = cross_val_predict(xgb_pipeline, \n                               train, \n                               y=train_y,\n                               method='predict_proba')\ntrain_pred = train_pred[:,1] #only want p(default)\n\n# Show calibration curve\nfraction_of_positives, mean_predicted_value = \\\n    calibration_curve(train_y, train_pred, n_bins=10)\nplt.figure()\nplt.plot([0, 1], [0, 1], 'k:', \n         label='Perfectly Calibrated')\nplt.plot(mean_predicted_value, \n         fraction_of_positives, 's-',\n         label='XGBoost Predictions')\nplt.legend()\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration curve for baseline XGBoost model')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a91eeb0cc697aa35f96e779ca53ab603d8681c6"
      },
      "cell_type": "markdown",
      "source": "The model is pretty well calibrated as is, exept for at higher predicted probabilities.  We can \"calibrate\" our model by adjusting predicted probabilities to more accurately reflect the probability of loan default by calibrating the model's predictions using a method based on isotonic regression."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b19260e3e782c46d25349acc700b95d50b8eaf3"
      },
      "cell_type": "code",
      "source": "# Classification pipeline w/ calibration\ncalib_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', CalibratedClassifierCV(\n                        base_estimator=XGBClassifier(),\n                        method='isotonic'))\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea1cf2f6cf67d8f4ac02d47b63407b1e38c2d0d7"
      },
      "cell_type": "code",
      "source": "# Predict probabilities for training data\ncalib_pred = cross_val_predict(calib_pipeline, \n                               train, \n                               y=train_y,\n                               method='predict_proba')\ncalib_pred = calib_pred[:,1] #only want p(default)\n\n# Show calibration curve\nfop_calib, mpv_calib = \\\n    calibration_curve(train_y, calib_pred, n_bins=10)\nfop_sig, mpv_sig = \\\n    calibration_curve(train_y, sig_pred, n_bins=10)\nplt.figure()\nplt.plot([0, 1], [0, 1], 'k:', \n         label='Perfectly Calibrated')\nplt.plot(mean_predicted_value, \n         fraction_of_positives, 's-',\n         label='XGBoost Predictions')\nplt.plot(mpv_calib, fop_calib, 's-',\n         label='Calibrated Predictions')\nplt.legend()\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration curve for Calibrated XGBoost model')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c5fc4cc8555d7411e5cb5e35ce93e9c86e56a637"
      },
      "cell_type": "markdown",
      "source": "Now we can output our predictions after calibrating."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2e32d52ae44dd8a8999348efc46b51a51c47727"
      },
      "cell_type": "code",
      "source": "# Cross-validated AUROC\nauroc_scorer = make_scorer(roc_auc_score, needs_proba=True)\nscores = cross_val_score(calib_pipeline, train, train_y, \n                         cv=3, scoring=auroc_scorer)\nprint('Mean AUROC with calibration:', scores.mean())\n\n# Fit to training data\ncalib_fit = calib_pipeline.fit(train, train_y)\n\n# Predict default probabilities of test data\ntest_pred = calib_fit.predict_proba(test)\n\n# Save predictions to file\ndf_out = pd.DataFrame()\ndf_out['SK_ID_CURR'] = test.index\ndf_out['TARGET'] = test_pred[:,1]\ndf_out.to_csv('xgboost_calibrated.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "Notes:\n\nFeature selection with Mutual information?  Featuretools?\n\ninteraction terms and keep ones w/ high MI?  Poly features shouldn't be needed for tree method... but would want em if doing logistic reg.\n\nGonna have to do oversampling/undersampling to account for that class imbalance\n\nXGboost\n\nCALIBRATION.  Tried sigmoid calibration but was even worse than isotonic.\n\nBayesian paramater optimization?"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}