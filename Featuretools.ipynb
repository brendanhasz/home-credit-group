{
  "cells": [
    {
      "metadata": {
        "_uuid": "be4820cca6d91fa5b86e64729a28be8529eb4b9d"
      },
      "cell_type": "markdown",
      "source": "# Automated Feature Engineering with Featuretools\n\n[Featuretools](https://www.featuretools.com/) is a fantastic python package for automated feature engineering.  It can automatically generate features from secondary datasets which can then be used in machine learning models.  In this post we'll see how automated feature engineering with Featuretools works, and how to run it on complex multi-table datsets!\n\n**Outline**\n- [Automated Feature Engineering](#automated-feature-engineering)\n- [Deep Feature Synthesis](#deep-feature-synthesis)\n- [Using Featuretools](#using-featuretools)\n- [Predictions from Generated Features](#predictions-from-generated-features)\n- [Running out of Memory](#running-out-of-memory)\n"
    },
    {
      "metadata": {
        "_uuid": "6dd2fc2e491413c6703e26bc2c1b9ccd1a51f002"
      },
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"automated-feature-engineering\"></a>\n## Automated Feature Engineering\n\nWhat do I mean by \"automated feature engineering\" and how is it useful?  When building predictive models, we need to have training examples which have some set of features.  For most machine learning algorithms (though of course not all of them), this training set needs to take the form of a table or matrix, where each row corresponds to a single training example or observation, and each column corresponds to a different feature.  For example, suppose we're trying to predict how likely loan applicants are to successfully repay their loans.  In this case, our data table will have a row for each applicant, and a column for each \"feature\" of the applicants, such as their income, their current level of credit, their age, etc.\n\nUnfortunately, in most applications the data isn't quite as simple as just one table.  We'll likely have additional data stored in other tables!  To continue with the loan repayment prediction example, we could have a separate table which stores the monthly balances of applicants on their other loans, and another separate table with the credit card accounts for each applicant, and yet another table with the credit card activity for each of those accounts, and so on. \n\n![Data table tree](/assets/img/featuretools/DataframeTree.svg)\n\nIn order to build a predictive model, we need to \"engineer\" features from data in those secondary tables.  These engineered features can then be added to our main data table, which we can then use to train the predictive model.  For example, we could compute the number of credit card accounts for each applicant, and add that as a feature to our primary data table; we could compute the balance across each applicant's credit cards, and add that to the primary data table; we could also compute the balance to available credit ratio and add that as a feature; etc.\n\nWith complicated (read: real-life) datasets, the number of features that we could engineer becomes very large, and the task of manually engineering all these features becomes extremely time-intensive.  The [Featuretoools](https://www.featuretools.com/) package automates this process by automatically generating features for our primary data table from information in secondary data sources."
    },
    {
      "metadata": {
        "_uuid": "5c509a59e3f07f805ca588468df93876dd7f2485"
      },
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"deep-feature-synthesis\"></a>\n## Deep Feature Synthesis\n\n**TODO**: explain deep feature synthesis, feature primitives, etc"
    },
    {
      "metadata": {
        "_uuid": "0871909c6cd61753d917ba9e32d12a283b2bf440"
      },
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"using-featuretools\"></a>\n## Using Featuretools\n\n**TODO**: show how to use it after loading data in w/ pandas, and then run example model on it (e.g. lightGBM)\n\nTo show how Featuretools works, we'll be using it on the [Home Credit Group Default Risk](https://www.kaggle.com/c/home-credit-default-risk/data) dataset.  This dataset contains information about individuals applying for loans with [Home Credit Group](http://www.homecredit.net/), a consumer lender specializing in loans to individuals with little credit history.  Home Credit Group hopes to be able to predict how likely an applicant is to default on their loan, in order to decide whether a given loan plan is good for a specific applicant (or whether to suggest a different payment schedule).  \n\nThe dataset contains multiple tables which relate to one another in some way.  Below is a diagram which shows each data table, the information it contains, and how each table is related to each other table.\n\n![File connection columns](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n\nThe primary tables (`application_train.csv` and `application_test.csv`) have information on each of the loan applications, where each row corresponds to a single application.  The train table has information about whether that applicant ended up defaulting on their loan, while the test table does not (because those are the applications we'll be testing our predictive model on).  The other tables contain information about other loans (either at other institutions, in the `bureau.csv` and `bureau_balance.csv` tables, or previous loans with Home Credit, in `previous_applications.csv`, `POS_CASH_balance.csv`, `instalments_payments.csv`, and `credit_card_balance.csv`).\n\nWhat are the relationships between these tables?  The value in the `SK_ID_CURR` column of the `application_*.csv` and `bureau.csv` tables identify the applicant.  That is, to combine the two tables into a single table, we could merge on `SK_ID_CURR`.   Similarly, the `SK_ID_BUREAU` column in `bureau.csv` and `bureau_balance.csv` identifies the applicant, though in this case there can be multiple entries in `bureau_balance.csv` for a single applicant.  The text in the line connecting the tables in the diagram above shows what column two tables can be merged on.\n\nWe could manually go through all these databases and construct features based on them, but this would entail not just a lot of manual work, but a *lot* of design decisions.  For example, should we construct a feature which corresponds to the maximum amount of credit the applicant has ever carried?  Or the average amount of credit?  Or the monthly median credit?  Should we construct a feature for how many payments the applicant has made, or how regular their payments are, or *when* they make their payments, etc, etc, etc? \n\nFeaturetools allows us to define our datasets, the relationships between our datasets, and automatically extracts features from child datasets into parent datasets using deep feature synthesis.  We'll use Featuretools to generate features from the data in the secondary tables in the Home Credit Group dataset, and keep features which are informative. \n\nFirst let's load the packages we need."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom lightgbm import LGBMClassifier\nimport featuretools as ft\nfrom featuretools import selection",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5402661119519f4267bfbd3d53713ed2c1aa73bb"
      },
      "cell_type": "markdown",
      "source": "We'll use pandas to load the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbafb8dcf1af4e7d9c3b4bf16161327a6ae695dd"
      },
      "cell_type": "code",
      "source": "# Load applications data\nNrows = 10000\ntrain = pd.read_csv('../input/application_train.csv', nrows=Nrows)\ntest = pd.read_csv('../input/application_test.csv', nrows=Nrows)\nbureau = pd.read_csv('../input/bureau.csv', nrows=Nrows)\nbureau_balance = pd.read_csv('../input/bureau_balance.csv', nrows=Nrows)\ncash_balance = pd.read_csv('../input/POS_CASH_balance.csv', nrows=Nrows)\ncard_balance = pd.read_csv('../input/credit_card_balance.csv', nrows=Nrows)\nprev_app = pd.read_csv('../input/previous_application.csv', nrows=Nrows)\npayments = pd.read_csv('../input/installments_payments.csv', nrows=Nrows)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "64c8ad85ea70fbcbc4474b0000ae9a6c39645167"
      },
      "cell_type": "markdown",
      "source": "To ensure that featuretools creates the same features for the test set as for the training set, we'll merge the two tables, but add a column which indicates whether each row is a test or training "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "569b63d0d9d83e59de73fb8dbcbf98c70a51ad7f"
      },
      "cell_type": "code",
      "source": "# Merge application data\ntrain['Test'] = False\ntest['Test'] = True\ntest['TARGET'] = np.nan\napp = train.append(test, ignore_index=True, sort=False)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8a99dd1311d2d7e2349c39b89f46db6038407333"
      },
      "cell_type": "markdown",
      "source": "Now we can take a look at the data in the main table."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1069148a1059e961d206762a7dda6df9b5c27017"
      },
      "cell_type": "code",
      "source": "app.sample(10)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "       SK_ID_CURR  TARGET  ...   AMT_REQ_CREDIT_BUREAU_YEAR   Test\n9371       110897     0.0  ...                          0.0  False\n15127      137286     NaN  ...                          2.0   True\n19734      170707     NaN  ...                          2.0   True\n8556       109961     0.0  ...                          NaN  False\n806        100923     0.0  ...                          3.0  False\n17873      157859     NaN  ...                          1.0   True\n18679      163386     NaN  ...                          NaN   True\n4087       104782     0.0  ...                          0.0  False\n13028      121227     NaN  ...                          0.0   True\n15817      142590     NaN  ...                          2.0   True\n\n[10 rows x 123 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>TARGET</th>\n      <th>NAME_CONTRACT_TYPE</th>\n      <th>CODE_GENDER</th>\n      <th>FLAG_OWN_CAR</th>\n      <th>FLAG_OWN_REALTY</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>AMT_GOODS_PRICE</th>\n      <th>NAME_TYPE_SUITE</th>\n      <th>NAME_INCOME_TYPE</th>\n      <th>NAME_EDUCATION_TYPE</th>\n      <th>NAME_FAMILY_STATUS</th>\n      <th>NAME_HOUSING_TYPE</th>\n      <th>REGION_POPULATION_RELATIVE</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>DAYS_REGISTRATION</th>\n      <th>DAYS_ID_PUBLISH</th>\n      <th>OWN_CAR_AGE</th>\n      <th>FLAG_MOBIL</th>\n      <th>FLAG_EMP_PHONE</th>\n      <th>FLAG_WORK_PHONE</th>\n      <th>FLAG_CONT_MOBILE</th>\n      <th>FLAG_PHONE</th>\n      <th>FLAG_EMAIL</th>\n      <th>OCCUPATION_TYPE</th>\n      <th>CNT_FAM_MEMBERS</th>\n      <th>REGION_RATING_CLIENT</th>\n      <th>REGION_RATING_CLIENT_W_CITY</th>\n      <th>WEEKDAY_APPR_PROCESS_START</th>\n      <th>HOUR_APPR_PROCESS_START</th>\n      <th>REG_REGION_NOT_LIVE_REGION</th>\n      <th>REG_REGION_NOT_WORK_REGION</th>\n      <th>LIVE_REGION_NOT_WORK_REGION</th>\n      <th>REG_CITY_NOT_LIVE_CITY</th>\n      <th>REG_CITY_NOT_WORK_CITY</th>\n      <th>LIVE_CITY_NOT_WORK_CITY</th>\n      <th>...</th>\n      <th>LIVINGAREA_MEDI</th>\n      <th>NONLIVINGAPARTMENTS_MEDI</th>\n      <th>NONLIVINGAREA_MEDI</th>\n      <th>FONDKAPREMONT_MODE</th>\n      <th>HOUSETYPE_MODE</th>\n      <th>TOTALAREA_MODE</th>\n      <th>WALLSMATERIAL_MODE</th>\n      <th>EMERGENCYSTATE_MODE</th>\n      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n      <th>DAYS_LAST_PHONE_CHANGE</th>\n      <th>FLAG_DOCUMENT_2</th>\n      <th>FLAG_DOCUMENT_3</th>\n      <th>FLAG_DOCUMENT_4</th>\n      <th>FLAG_DOCUMENT_5</th>\n      <th>FLAG_DOCUMENT_6</th>\n      <th>FLAG_DOCUMENT_7</th>\n      <th>FLAG_DOCUMENT_8</th>\n      <th>FLAG_DOCUMENT_9</th>\n      <th>FLAG_DOCUMENT_10</th>\n      <th>FLAG_DOCUMENT_11</th>\n      <th>FLAG_DOCUMENT_12</th>\n      <th>FLAG_DOCUMENT_13</th>\n      <th>FLAG_DOCUMENT_14</th>\n      <th>FLAG_DOCUMENT_15</th>\n      <th>FLAG_DOCUMENT_16</th>\n      <th>FLAG_DOCUMENT_17</th>\n      <th>FLAG_DOCUMENT_18</th>\n      <th>FLAG_DOCUMENT_19</th>\n      <th>FLAG_DOCUMENT_20</th>\n      <th>FLAG_DOCUMENT_21</th>\n      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n      <th>Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9371</th>\n      <td>110897</td>\n      <td>0.0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>157500.0</td>\n      <td>755190.0</td>\n      <td>36459.0</td>\n      <td>675000.0</td>\n      <td>Family</td>\n      <td>Commercial associate</td>\n      <td>Higher education</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>0.009657</td>\n      <td>-11106</td>\n      <td>-935</td>\n      <td>-2499.0</td>\n      <td>-2376</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Laborers</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>TUESDAY</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>-741.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15127</th>\n      <td>137286</td>\n      <td>NaN</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>157500.0</td>\n      <td>296280.0</td>\n      <td>23539.5</td>\n      <td>225000.0</td>\n      <td>Unaccompanied</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Single / not married</td>\n      <td>House / apartment</td>\n      <td>0.024610</td>\n      <td>-10642</td>\n      <td>-1527</td>\n      <td>-4286.0</td>\n      <td>-2740</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Laborers</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>SATURDAY</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0313</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>block of flats</td>\n      <td>0.0242</td>\n      <td>Stone, brick</td>\n      <td>No</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-516.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>19734</th>\n      <td>170707</td>\n      <td>NaN</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>1</td>\n      <td>180000.0</td>\n      <td>632664.0</td>\n      <td>35325.0</td>\n      <td>540000.0</td>\n      <td>Unaccompanied</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Single / not married</td>\n      <td>House / apartment</td>\n      <td>0.018029</td>\n      <td>-14875</td>\n      <td>-2943</td>\n      <td>-3397.0</td>\n      <td>-3671</td>\n      <td>14.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Laborers</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>THURSDAY</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.2145</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>NaN</td>\n      <td>block of flats</td>\n      <td>0.1657</td>\n      <td>Stone, brick</td>\n      <td>No</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1610.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8556</th>\n      <td>109961</td>\n      <td>0.0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>3</td>\n      <td>135000.0</td>\n      <td>208854.0</td>\n      <td>22288.5</td>\n      <td>184500.0</td>\n      <td>Family</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>0.035792</td>\n      <td>-13462</td>\n      <td>-1085</td>\n      <td>-2303.0</td>\n      <td>-2307</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sales staff</td>\n      <td>5.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>TUESDAY</td>\n      <td>17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>806</th>\n      <td>100923</td>\n      <td>0.0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>2</td>\n      <td>270000.0</td>\n      <td>284256.0</td>\n      <td>30289.5</td>\n      <td>270000.0</td>\n      <td>Family</td>\n      <td>Commercial associate</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>0.007114</td>\n      <td>-12333</td>\n      <td>-502</td>\n      <td>-2571.0</td>\n      <td>-3232</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Cleaning staff</td>\n      <td>4.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>THURSDAY</td>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0423</td>\n      <td>NaN</td>\n      <td>0.0305</td>\n      <td>NaN</td>\n      <td>block of flats</td>\n      <td>0.0392</td>\n      <td>Stone, brick</td>\n      <td>No</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>-911.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17873</th>\n      <td>157859</td>\n      <td>NaN</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>1</td>\n      <td>270000.0</td>\n      <td>1096020.0</td>\n      <td>56092.5</td>\n      <td>900000.0</td>\n      <td>Unaccompanied</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>0.011657</td>\n      <td>-12583</td>\n      <td>-1231</td>\n      <td>-6400.0</td>\n      <td>-2250</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Managers</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>SATURDAY</td>\n      <td>17</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>-661.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>18679</th>\n      <td>163386</td>\n      <td>NaN</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>675000.0</td>\n      <td>25447.5</td>\n      <td>675000.0</td>\n      <td>Family</td>\n      <td>State servant</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>With parents</td>\n      <td>0.018029</td>\n      <td>-20274</td>\n      <td>-1629</td>\n      <td>-2516.0</td>\n      <td>-2775</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Drivers</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>WEDNESDAY</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1424.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4087</th>\n      <td>104782</td>\n      <td>0.0</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>0</td>\n      <td>270000.0</td>\n      <td>932643.0</td>\n      <td>27400.5</td>\n      <td>778500.0</td>\n      <td>Unaccompanied</td>\n      <td>Pensioner</td>\n      <td>Secondary / secondary special</td>\n      <td>Single / not married</td>\n      <td>House / apartment</td>\n      <td>0.003122</td>\n      <td>-19971</td>\n      <td>365243</td>\n      <td>-8071.0</td>\n      <td>-3489</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>TUESDAY</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.0949</td>\n      <td>0.0</td>\n      <td>0.0022</td>\n      <td>reg oper account</td>\n      <td>block of flats</td>\n      <td>0.0628</td>\n      <td>Panel</td>\n      <td>No</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>-1579.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13028</th>\n      <td>121227</td>\n      <td>NaN</td>\n      <td>Cash loans</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>1</td>\n      <td>90000.0</td>\n      <td>225000.0</td>\n      <td>9589.5</td>\n      <td>225000.0</td>\n      <td>NaN</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Civil marriage</td>\n      <td>Municipal apartment</td>\n      <td>0.026392</td>\n      <td>-15458</td>\n      <td>-5553</td>\n      <td>-5122.0</td>\n      <td>-4681</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>High skill tech staff</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>MONDAY</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>15817</th>\n      <td>142590</td>\n      <td>NaN</td>\n      <td>Cash loans</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>312840.0</td>\n      <td>24844.5</td>\n      <td>247500.0</td>\n      <td>Unaccompanied</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>0.010643</td>\n      <td>-11505</td>\n      <td>-1258</td>\n      <td>-1212.0</td>\n      <td>-1252</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>TUESDAY</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1067.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "c83bc5b0363e361864d48367e1b416fa37254300"
      },
      "cell_type": "code",
      "source": "# Print info about each column in the dataset\nfor col in app:\n    print(col)\n    Nnan = train[col].isnull().sum()\n    print('Number empty: ', Nnan)\n    print('Percent empty: ', 100*Nnan/train.shape[0])\n    print(train[col].describe())\n    if train[col].dtype==object:\n        print('Categories and Count:')\n        print(train[col].value_counts().to_string(header=None))\n    print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc7177f28a14c85162c608d9e85bee10ffe432a4"
      },
      "cell_type": "code",
      "source": "for col in test:\n    if test[col].dtype==object:\n        print(col)\n        print('Num Unique in Train:', train[col].nunique())\n        print('Num Unique in Test: ', test[col].nunique())\n        print('Unique in Train:', sorted([str(e) for e in train[col].unique().tolist()]))\n        print('Unique in Test: ', sorted([str(e) for e in test[col].unique().tolist()]))\n        print()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "45a690fa11a3f40319fbfe0d11febf80bdb84202"
      },
      "cell_type": "markdown",
      "source": "The first step in using Featuretools is to define the \"entities\", each of which is one data file or table, and the columns along which they are indexed."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec78afa14c429bc7b66ddccc7bd4ea6d0b993bd0"
      },
      "cell_type": "code",
      "source": "# Define entities\n# Each entry is \"Name\", (df, \"id_col_name\")\nentities = { #use id_col_name not in df for new index, w/ None uses 1st col\n    'app': (app, 'SK_ID_CURR'),\n    'bureau': (bureau, 'SK_ID_BUREAU'),\n    'bureau_balance': (bureau_balance, 'New'),\n    'cash_balance': (cash_balance, 'New'),\n    'card_balance': (card_balance, 'New'),\n    'prev_app': (prev_app, 'SK_ID_PREV'),\n    'payments': (payments, 'New') \n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "304e96216d96164e2c6cfa7d8268c41d2c59feb5"
      },
      "cell_type": "markdown",
      "source": "Next we'll define the relationships between these entities (how each row in one dataset relates to a row in another dataset, based of an ID value in a given column for each entity)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12c849c9a6770387d2f46be550ec11d4b84a0889"
      },
      "cell_type": "code",
      "source": "# Define relationships between dataframes\n# Each entry is (parent_entity, parent_variable, child_entity, child_variable)\nrelationships = [\n    ('app', 'SK_ID_CURR', 'bureau', 'SK_ID_CURR'),\n    ('bureau', 'SK_ID_BUREAU', 'bureau_balance', 'SK_ID_BUREAU'),\n    ('app', 'SK_ID_CURR', 'prev_app', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'cash_balance', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'payments', 'SK_ID_CURR'),\n    ('app', 'SK_ID_CURR', 'card_balance', 'SK_ID_CURR')\n]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "544399c1ec67a471e3c82e0f3d3f653012e2e8cd"
      },
      "cell_type": "markdown",
      "source": "Then we'll define which \"feature primitives\" we want to use to construct features.  First let's look at a list of all the feature primitives available in Featuretools:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d9c1f6c022a6971e14e5873a41c3f781f8c6dcf"
      },
      "cell_type": "code",
      "source": "pd.options.display.max_rows = 100\nft.list_primitives()",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "                   name                        ...                                                                description\n0                 trend                        ...                          Calculates the slope of the linear trend of va...\n1                   sum                        ...                          Counts the number of elements of a numeric or ...\n2                median                        ...                          Finds the median value of any feature with wel...\n3       time_since_last                        ...                                          Time since last related instance.\n4                 count                        ...                                      Counts the number of non null values.\n5      avg_time_between                        ...                          Computes the average time between consecutive ...\n6          percent_true                        ...                          Finds the percent of 'True' values in a boolea...\n7                   all                        ...                                             Test if all values are 'True'.\n8         n_most_common                        ...                          Finds the N most common elements in a categori...\n9              num_true                        ...                            Finds the number of 'True' values in a boolean.\n10                  any                        ...                                               Test if any value is 'True'.\n11           num_unique                        ...                          Returns the number of unique categorical varia...\n12                 last                        ...                                                    Returns the last value.\n13                  max                        ...                          Finds the maximum non-null value of a numeric ...\n14                 mode                        ...                          Finds the most common element in a categorical...\n15                  std                        ...                          Finds the standard deviation of a numeric feat...\n16                  min                        ...                          Finds the minimum non-null value of a numeric ...\n17                 mean                        ...                           Computes the average value of a numeric feature.\n18                 skew                        ...                                       Computes the skewness of a data set.\n19                hours                        ...                          Transform a Timedelta feature into the number ...\n20               minute                        ...                              Transform a Datetime feature into the minute.\n21              cum_sum                        ...                          Calculates the sum of previous values of an in...\n22                 days                        ...                          Transform a Timedelta feature into the number ...\n23             numwords                        ...                          Returns the words in a given string by countin...\n24                  and                        ...                          For two boolean values, determine if both valu...\n25                  mod                        ...                          Creates a transform feature that divides two f...\n26              weekday                        ...                          Transform Datetime feature into the boolean of...\n27             multiply                        ...                          Creates a transform feature that multplies two...\n28              seconds                        ...                          Transform a Timedelta feature into the number ...\n29  time_since_previous                        ...                              Compute the time since the previous instance.\n30                years                        ...                          Transform a Timedelta feature into the number ...\n31                  add                        ...                          Creates a transform feature that adds two feat...\n32           days_since                        ...                          For each value of the base feature, compute th...\n33              is_null                        ...                          For each value of base feature, return 'True' ...\n34               months                        ...                          Transform a Timedelta feature into the number ...\n35            longitude                        ...                          Returns the second value on the tuple base fea...\n36                weeks                        ...                          Transform a Timedelta feature into the number ...\n37           percentile                        ...                          For each value of the base feature, determines...\n38              cum_max                        ...                          Calculates the max of previous values of an in...\n39               second                        ...                              Transform a Datetime feature into the second.\n40              minutes                        ...                          Transform a Timedelta feature into the number ...\n41                 diff                        ...                          Compute the difference between the value of a ...\n42             cum_mean                        ...                          Calculates the mean of previous values of an i...\n43                 hour                        ...                                Transform a Datetime feature into the hour.\n44               negate                        ...                          Creates a transform feature that negates a fea...\n45                   or                        ...                          For two boolean values, determine if one value...\n46                  day                        ...                                 Transform a Datetime feature into the day.\n47           characters                        ...                                   Return the characters in a given string.\n48               divide                        ...                          Creates a transform feature that divides two f...\n49              weekend                        ...                          Transform Datetime feature into the boolean of...\n50             subtract                        ...                          Creates a transform feature that subtracts two...\n51                 isin                        ...                          For each value of the base feature, checks whe...\n52             absolute                        ...                                            Absolute value of base feature.\n53                 year                        ...                                Transform a Datetime feature into the year.\n54            haversine                        ...                          Calculate the approximate haversine distance i...\n55           time_since                        ...                                     Calculates time since the cutoff time.\n56                month                        ...                               Transform a Datetime feature into the month.\n57             latitude                        ...                          Returns the first value of the tuple base feat...\n58              cum_min                        ...                          Calculates the min of previous values of an in...\n59                 week                        ...                                Transform a Datetime feature into the week.\n60                  not                        ...                          For each value of the base feature, negates th...\n61            cum_count                        ...                          Calculates the number of previous values of an...\n\n[62 rows x 3 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>type</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>trend</td>\n      <td>aggregation</td>\n      <td>Calculates the slope of the linear trend of va...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sum</td>\n      <td>aggregation</td>\n      <td>Counts the number of elements of a numeric or ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>median</td>\n      <td>aggregation</td>\n      <td>Finds the median value of any feature with wel...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>time_since_last</td>\n      <td>aggregation</td>\n      <td>Time since last related instance.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>count</td>\n      <td>aggregation</td>\n      <td>Counts the number of non null values.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>avg_time_between</td>\n      <td>aggregation</td>\n      <td>Computes the average time between consecutive ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>percent_true</td>\n      <td>aggregation</td>\n      <td>Finds the percent of 'True' values in a boolea...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>all</td>\n      <td>aggregation</td>\n      <td>Test if all values are 'True'.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>n_most_common</td>\n      <td>aggregation</td>\n      <td>Finds the N most common elements in a categori...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>num_true</td>\n      <td>aggregation</td>\n      <td>Finds the number of 'True' values in a boolean.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>any</td>\n      <td>aggregation</td>\n      <td>Test if any value is 'True'.</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>num_unique</td>\n      <td>aggregation</td>\n      <td>Returns the number of unique categorical varia...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>last</td>\n      <td>aggregation</td>\n      <td>Returns the last value.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>max</td>\n      <td>aggregation</td>\n      <td>Finds the maximum non-null value of a numeric ...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>mode</td>\n      <td>aggregation</td>\n      <td>Finds the most common element in a categorical...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>std</td>\n      <td>aggregation</td>\n      <td>Finds the standard deviation of a numeric feat...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>min</td>\n      <td>aggregation</td>\n      <td>Finds the minimum non-null value of a numeric ...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>mean</td>\n      <td>aggregation</td>\n      <td>Computes the average value of a numeric feature.</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>skew</td>\n      <td>aggregation</td>\n      <td>Computes the skewness of a data set.</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>hours</td>\n      <td>transform</td>\n      <td>Transform a Timedelta feature into the number ...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>minute</td>\n      <td>transform</td>\n      <td>Transform a Datetime feature into the minute.</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>cum_sum</td>\n      <td>transform</td>\n      <td>Calculates the sum of previous values of an in...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>days</td>\n      <td>transform</td>\n      <td>Transform a Timedelta feature into the number ...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>numwords</td>\n      <td>transform</td>\n      <td>Returns the words in a given string by countin...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>and</td>\n      <td>transform</td>\n      <td>For two boolean values, determine if both valu...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>mod</td>\n      <td>transform</td>\n      <td>Creates a transform feature that divides two f...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>weekday</td>\n      <td>transform</td>\n      <td>Transform Datetime feature into the boolean of...</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>multiply</td>\n      <td>transform</td>\n      <td>Creates a transform feature that multplies two...</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>seconds</td>\n      <td>transform</td>\n      <td>Transform a Timedelta feature into the number ...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>time_since_previous</td>\n      <td>transform</td>\n      <td>Compute the time since the previous instance.</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>years</td>\n      <td>transform</td>\n      <td>Transform a Timedelta feature into the number ...</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>add</td>\n      <td>transform</td>\n      <td>Creates a transform feature that adds two feat...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>days_since</td>\n      <td>transform</td>\n      <td>For each value of the base feature, compute th...</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>is_null</td>\n      <td>transform</td>\n      <td>For each value of base feature, return 'True' ...</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>months</td>\n      <td>transform</td>\n      <td>Transform a Timedelta feature into the number ...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>longitude</td>\n      <td>transform</td>\n      <td>Returns the second value on the tuple base fea...</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>weeks</td>\n      <td>transform</td>\n      <td>Transform a Timedelta feature into the number ...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>percentile</td>\n      <td>transform</td>\n      <td>For each value of the base feature, determines...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>cum_max</td>\n      <td>transform</td>\n      <td>Calculates the max of previous values of an in...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>second</td>\n      <td>transform</td>\n      <td>Transform a Datetime feature into the second.</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>minutes</td>\n      <td>transform</td>\n      <td>Transform a Timedelta feature into the number ...</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>diff</td>\n      <td>transform</td>\n      <td>Compute the difference between the value of a ...</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>cum_mean</td>\n      <td>transform</td>\n      <td>Calculates the mean of previous values of an i...</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>hour</td>\n      <td>transform</td>\n      <td>Transform a Datetime feature into the hour.</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>negate</td>\n      <td>transform</td>\n      <td>Creates a transform feature that negates a fea...</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>or</td>\n      <td>transform</td>\n      <td>For two boolean values, determine if one value...</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>day</td>\n      <td>transform</td>\n      <td>Transform a Datetime feature into the day.</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>characters</td>\n      <td>transform</td>\n      <td>Return the characters in a given string.</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>divide</td>\n      <td>transform</td>\n      <td>Creates a transform feature that divides two f...</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>weekend</td>\n      <td>transform</td>\n      <td>Transform Datetime feature into the boolean of...</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>subtract</td>\n      <td>transform</td>\n      <td>Creates a transform feature that subtracts two...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>isin</td>\n      <td>transform</td>\n      <td>For each value of the base feature, checks whe...</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>absolute</td>\n      <td>transform</td>\n      <td>Absolute value of base feature.</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>year</td>\n      <td>transform</td>\n      <td>Transform a Datetime feature into the year.</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>haversine</td>\n      <td>transform</td>\n      <td>Calculate the approximate haversine distance i...</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>time_since</td>\n      <td>transform</td>\n      <td>Calculates time since the cutoff time.</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>month</td>\n      <td>transform</td>\n      <td>Transform a Datetime feature into the month.</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>latitude</td>\n      <td>transform</td>\n      <td>Returns the first value of the tuple base feat...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>cum_min</td>\n      <td>transform</td>\n      <td>Calculates the min of previous values of an in...</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>week</td>\n      <td>transform</td>\n      <td>Transform a Datetime feature into the week.</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>not</td>\n      <td>transform</td>\n      <td>For each value of the base feature, negates th...</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>cum_count</td>\n      <td>transform</td>\n      <td>Calculates the number of previous values of an...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "de4f968135ce8eb2fef40a17dc52d4e67543324d"
      },
      "cell_type": "markdown",
      "source": "We'll use a simple set of feature primitives: just the mean and the count of entries in the secondary data files."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0709ebdc7d9a32ad377b4a4f9086885acfeccea9"
      },
      "cell_type": "code",
      "source": "# Define which primitives to use\nagg_primitives =  ['count', 'mean', 'num_unique', 'percent_true']\ntrans_primitives = ['time_since_previous']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "27ad8a5a61a66c57e694d84449567ecec504ac14"
      },
      "cell_type": "markdown",
      "source": "Finally, we can run deep feature synthesis on our entities given their relationships and a list of feature primitives."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6af0a95932e23095b68f317133f790b23b5a3de"
      },
      "cell_type": "code",
      "source": "# Run deep feature synthesis\nt0 = time.time()\ndfs_feat, dfs_defs = ft.dfs(entities=entities,\n                            relationships=relationships,\n                            target_entity='app',\n                            trans_primitives=trans_primitives,\n                            agg_primitives=agg_primitives, \n                            verbose = True,\n                            max_depth=2, n_jobs=2)\nprint('DFS took %0.3g sec' % (time.time()-t0))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "215c615d1945d94337cebd310ad62ba1ab36a379"
      },
      "cell_type": "code",
      "source": "# Delete things we don't need anymore\ngc.enable()\ndel train, test, app, bureau, bureau_balance, cash_balance, prev_app, payments\ngc.collect()\n\n# Use featuretools features\napp = dfs_feat",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bf2213a228a45b6ed972ce7215d00a5dff2554f"
      },
      "cell_type": "markdown",
      "source": "If we take a look at the dataframe which was returned by Featuretools, we can see that a bunch of features were appended which correspond to our selected feature primitive functions applied to data in the secondary data files which correspond to each row in the main application dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d49855795788d3205c015f06fbd5523546f77fe7"
      },
      "cell_type": "code",
      "source": "app",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "449b155cf051ac44fc2bb76c1ff20a50483abd48"
      },
      "cell_type": "markdown",
      "source": "Now that we've generated a bunch of features, we should make sure to remove ones which don't carry any information.  Featuretools includes a function to remove features which are entirely NULLs or only have one class, etc:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40f8cc6111003c60f474e7fce758a9d8430c601c"
      },
      "cell_type": "code",
      "source": "# Remove low information features\nNf0 = app.shape[1] #number of initial features\napp = selection.remove_low_information_features(dfs_feat)\nprint('Removed', Nf0-app.shape[1], 'features')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4440ad7b7a10147b88bc5acf0f1a55e5d353f3c6"
      },
      "cell_type": "markdown",
      "source": "In some cases it might also be a good idea to do further feature selection at this point, by, say, removing features which have low mutual information with the target variable (loan default)."
    },
    {
      "metadata": {
        "_uuid": "826e600e54c5fadc7cd65b89f36ee28d61a958eb"
      },
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"predictions-from-generated-features\"></a>\n## Predictions from Generated Features\n\nNow that we've generated features using Featuretools, we can use those generated features in a predictive model.  First we have to split our features back into training and test datasets, and remove the indicator columns."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f78117d7175b1486f4907ff0b3fd71d85a50496"
      },
      "cell_type": "code",
      "source": "# Split data back into test + train\ntrain = app.loc[~app['Test'], :]\ntest = app.loc[app['Test'], :]\n\n# Make SK_ID_CURR the index\ntrain.set_index('SK_ID_CURR', inplace=True)\ntest.set_index('SK_ID_CURR', inplace=True)\n\n# Ensure all data is stored as floats\ntrain = train.astype(np.float32)\ntest = test.astype(np.float32)\n\n# Target labels\ntrain_y = train['TARGET']\n\n# Remove test/train indicator column and target column\ntrain.drop(columns=['Test', 'TARGET'], inplace=True)\ntest.drop(columns=['Test', 'TARGET'], inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ab4e248eda386ec34c5daf0c97cfbbad06d067f1"
      },
      "cell_type": "markdown",
      "source": "Then we can run a predictive model, such as LightGBM, on the generated features to predict how likely applicants are to default on their loans."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80aaa3097ea3b19dfab54a774d426b7e3e54840a"
      },
      "cell_type": "code",
      "source": "# Classification pipeline w/ LightGBM\nlgbm_pipeline = Pipeline([\n    ('scaler', RobustScaler()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('classifier', CalibratedClassifierCV(\n                        base_estimator=LGBMClassifier(),\n                        method='isotonic'))\n])\n\n# Fit to training data\nlgbm_fit = lgbm_pipeline.fit(train, train_y)\n\n# Predict loan default probabilities of test data\ntest_pred = lgbm_fit.predict_proba(test)\n\n# Save predictions to file\ndf_out = pd.DataFrame()\ndf_out['SK_ID_CURR'] = test.index\ndf_out['TARGET'] = test_pred[:,1]\ndf_out.to_csv('test_predictions.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5225207564540f0f72f89efb54c6da209826b7d6"
      },
      "cell_type": "markdown",
      "source": "<a class=\"anchor\" id=\"running-out-of-memory\"></a>\n## Running out of Memory\n\nThe downside of Featuretools is that is isn't generating features all that intelligently - it simply generates features by applying all the feature primitives to all the features in secondary datasets recursively.  This means that the number of features which are generated can be *huge*!  When dealing with large datasets, this means that the feature generation process might take up more memory than is available on a personal computer.  If you run out of memory, you can always [run featuretools on an Amazon Web Services EC2 instance] (https://brendanhasz.github.io/2018/08/30/aws.html) which has enough memory, such as the `r5` class of instances."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}